<main class="calibre3">

## 第三章 计算机体系结构

</main>

<main class="calibre3">

## 1 CPU

<picture>![](img/computer.svg)</picture>

CPU 对寄存器中持有的值执行指令。这个例子首先将 R1 的值设置为 100，将内存位置 0x100 中的值加载到 R2，将两个值相加并将结果放在 R3 中，最后将新的值（110）存储到 R4（供进一步使用）。

图 1.1 CPU

简而言之，计算机由一个连接到内存的中央处理单元（CPU）组成。上面的图示说明了所有计算机操作背后的基本原理。

CPU 执行从内存中读取的指令。指令分为两类

1.  那些从内存中将值加载到寄存器中，并将值从寄存器存储到内存中的。

1.  那些在寄存器中存储的值上操作的。例如，在两个寄存器中添加、减去、乘以或除以值，执行位操作（与、或、异或等）或执行其他数学操作（平方根、正弦、余弦、正切等）。

在这个例子中，我们只是将存储在内存中的值加上 100，并将这个新的结果存储回内存。

### 1.1 分支

除了加载或存储之外，CPU 的另一个重要操作是*分支*。内部，CPU 记录下要执行的下一个指令的*指令指针*。通常，指令指针会增加以指向下一个指令的顺序；分支指令通常会检查特定寄存器是否为零或标志是否设置，如果是，则会将指针修改为不同的地址。因此，要执行的下一个指令将来自程序的另一部分；这就是循环和决策语句是如何工作的。

例如，一个像`if (x==0)`这样的语句可能通过找到两个寄存器的`or`来实现，一个寄存器持有`x`，另一个为零；如果结果是零，比较为真（即`x`的所有位都是零），则应采取语句的主体，否则跳过主体代码。

### 1.2 循环

我们都熟悉计算机的速度，以兆赫兹或吉赫兹（每秒百万或十亿个周期）表示。这被称为*时钟速度*，因为它是指计算机内部时钟脉冲的速度。

脉冲在处理器内部用于保持内部同步。在每个滴答或脉冲之后，可以开始另一个操作；想想看，时钟就像敲鼓的人，以保持划船者的桨同步。

### 1.3 取指令、解码、执行、存储

执行单个指令由一系列特定的事件周期组成；取指令、解码、执行和存储。

例如，要执行上面的`add`指令，CPU 必须

1.  取指令：将指令从内存中获取到处理器中。

1.  解码：内部解码它必须做什么（在这种情况下是添加）。

1.  执行：从寄存器中取值，实际上将它们相加

1.  存储：将结果存储回另一个寄存器。您也可能看到术语*退休*指令。

#### 1.3.1 查看 CPU 内部结构

内部，CPU 有许多不同的子组件，执行上述每一步，并且通常它们都可以相互独立地发生。这类似于一个物理生产线，其中有许多站点，每个站点都有特定的任务要执行。一旦完成，它可以将结果传递给下一个站点，并接收新的输入来处理。

<picture>![](img/block.svg)</picture>

CPU 由许多不同的子组件组成，每个组件都执行特定的任务。

图 1.3.1.1 CPU 内部结构

图 1.3.1.1，CPU 内部结构展示了一个非常简单的框图，说明了现代 CPU 的一些主要部分。

您可以看到指令进入并被处理器解码。CPU 有两种主要类型的寄存器，一种是用于*整数*计算的寄存器，另一种是用于*浮点*计算的寄存器。浮点是一种用二进制形式表示带小数点的数字的方法，并在 CPU 内部以不同的方式处理。*MMX*（多媒体扩展）和*SSE*（流式单指令多数据）或*Altivec*寄存器与浮点寄存器类似。

*寄存器文件*是 CPU 内部寄存器的总称。在其下方，我们有 CPU 真正执行所有工作的部分。

我们说过，处理器要么将值加载到寄存器中，要么从寄存器中存储到内存中，要么对寄存器中的值执行某些操作。

*算术逻辑单元*（ALU）是 CPU 操作的核心。它从寄存器中取值并执行 CPU 能够执行的各种操作。所有现代处理器都有多个 ALU，因此它们可以独立工作。实际上，像 Pentium 这样的处理器既有*快速*ALU 也有*慢速*ALU；快速的 ALU 更小（因此可以在 CPU 上放置更多），但只能执行最常见的操作，慢速 ALU 可以执行所有操作但体积更大。

*地址生成单元*（AGU）负责与缓存和主内存通信，将值放入寄存器以供 ALU 操作，并将值从寄存器中取出放入主内存。

浮点寄存器有相同的概念，但它们的组件使用略不同的术语。

#### 1.3.2 流水线

正如我们上面所看到的，虽然 ALU 在将寄存器相加与 AGU 将值写回内存是完全分开的，所以 CPU 没有理由不能同时做这两件事。我们系统中也有多个 ALU，每个都可以独立处理不同的指令。最后，CPU 可以在其浮点逻辑执行一些浮点操作的同时，整数指令也在进行中。这个过程被称为 *流水线*。实际上，任何现代处理器都可以流水线化更多阶段，上面我们只展示了一个非常简化的视图。可以同时执行更多阶段的处理器，其流水线更深，这样的处理器被称为 *超标量架构*。所有现代处理器都是超标量。

另一个类比可能就是想象流水线就像一个正在被弹珠填满的水管，只不过我们的弹珠是 CPU 的指令。理想情况下，你会一个接一个地将弹珠（指令）放入一端（每个时钟脉冲一个），填满管道。一旦填满，对于每个你推入的弹珠（指令），其他的都会移动到下一个位置，一个会从末端掉出（结果）。

分支指令对这个模型造成了破坏，因为它们可能或可能不会导致执行从不同的地方开始。如果你在流水线中，你将不得不基本上猜测分支将走向哪个方向，这样你才知道要将哪些指令带入流水线。如果 CPU 预测正确，一切都会顺利！例如，Pentium 这样的处理器使用 *跟踪缓存* 来跟踪分支的方向。很多时候，它可以通过记住其先前结果来预测分支将走向哪个方向。例如，在一个发生 100 次的循环中，如果你记得分支的最后结果，你将有 99 次是正确的，因为只有最后一次你实际上会继续程序。相反，如果处理器预测错误，它浪费了很多时间，不得不清空流水线并重新开始。

这个过程通常被称为 *流水线刷新*，就像不得不停下来并清空你水管中的所有弹珠一样！

##### 1.3.2.1 分支预测

流水线刷新，预测取，预测不取，分支延迟槽

#### 1.3.3 重新排序

事实上，如果 CPU 是水管，它可以在不改变弹珠出水的顺序的情况下自由地重新排列水管内的弹珠。我们称之为 *程序顺序*，因为这是计算机程序中指令给出的顺序。

```cpp
1 |
 |1: r3 = r1 * r2 
 |2: r4 = r2 + r3 
 |3: r7 = r5 * r6 
5 |4: r8 = r1 + r7 

```

图 1.3.3.1 重新排序缓冲区示例

考虑一个指令流，例如图 1.3.3.1，重排序缓冲区示例中所示。指令 2 需要等待指令 1 完全完成才能开始。这意味着流水线必须*暂停*，等待计算出的值。同样，指令 3 和 4 依赖于*r7*。然而，指令 2 和 3 之间没有任何*依赖*；这意味着它们操作在完全独立的寄存器上。如果我们交换指令 2 和 3，我们可以为流水线获得更好的排序，因为处理器可以执行有用的工作，而不是等待流水线完成以获取先前指令的结果。

然而，在编写非常底层的代码时，某些指令可能需要关于操作顺序的一些安全性。我们称这种要求为*内存语义*。如果你需要*获取*语义，这意味着你必须确保所有先前指令的结果都已经完成。如果你需要*释放*语义，这意味着你表示所有在此指令之后的指令都必须看到当前的结果。还有一种更严格的语义是*内存屏障*或*内存栅栏*，它要求在继续之前将操作提交到内存。

在某些体系结构中，这些语义由处理器保证，而在其他体系结构中，你必须明确指定它们。大多数程序员不需要直接担心这些，尽管你可能会看到这些术语。

### 1.4 CISC v RISC

将计算机体系结构划分为*复杂指令集计算机*（CISC）和*精简指令集计算机*（RISC）是一种常见的方法。

注意在第一个例子中，我们明确地将值加载到寄存器中，执行了加法操作，并将另一个寄存器中持有的结果值存储回内存。这是一个 RISC 计算方法的例子——只对寄存器中的值执行操作，并明确地将值从内存加载到寄存器或从寄存器存储到内存。

CISC 方法可能只是单个指令，从内存中获取值，在内部执行加法操作，并将结果写回。这意味着指令可能需要很多周期，但最终这两种方法都达到了相同的目标。

所有现代体系结构都可以被认为是 RISC 体系结构，即使是最常见的体系结构，如 Intel Pentium，虽然其指令集被归类为 CISC，但在执行之前，芯片内部将指令分解为 RISC 风格的子指令。

这有多个原因

+   虽然 RISC 使得汇编编程变得更加复杂，因为几乎所有程序员都使用高级语言，并将生成汇编代码的艰巨任务留给编译器，但其他优点超过了这一缺点。

+   由于 RISC 处理器的指令更加简单，芯片内部有更多的空间用于寄存器。正如我们从内存层次结构中所知，寄存器是最快的内存类型，最终所有指令都必须在寄存器中持有的值上执行，所以在其他条件相同的情况下，更多的寄存器会导致更高的性能。

+   由于所有指令都在相同的时间内执行，因此流水线是可能的。我们知道流水线需要不断将指令流输入到处理器中，所以如果某些指令执行时间非常长，而其他指令则不，那么流水线就会变得过于复杂而无法有效工作。

#### 1.4.1 EPIC

在本书的许多例子中使用的 Itanium 处理器是修改后的架构，称为显式并行指令计算（EPIC）的例子。

我们已经讨论了超标量处理器如何具有流水线，在处理器的不同部分同时有多个指令在飞行。显然，为了尽可能好地工作，应该按照可以最好利用 CPU 可用元素的顺序向处理器提供指令。

传统上，组织传入的指令流是硬件的工作。程序以顺序方式发出指令；处理器必须向前看并尝试就如何组织传入的指令做出决定。

EPIC 背后的理论是，在更高的层面上有更多的信息可用，这可以使这些决策比处理器更好。像当前处理器那样分析汇编语言指令流，会丢失程序员在原始源代码中可能提供的大量信息。把它想象成研究莎士比亚的戏剧和阅读同一戏剧的 Cliff's Notes 版本之间的区别。两者都给出了相同的结果，但原始版本有各种各样的额外信息，这些信息设定了场景，并让你对角色有了深入了解。

因此，指令排序的逻辑可以从处理器移动到编译器。这意味着编译器编写者需要更聪明，以尝试找到最适合处理器的代码排序。处理器本身也显著简化了，因为大量工作已经转移到编译器上。

在 EPIC 周围经常使用的另一个术语是超长指令字（VLIW），这里的每个处理器指令都被扩展以告诉处理器在它的内部单元中应该在哪里执行指令。这种方法的问题在于，代码随后完全依赖于为它编译的处理器模型。公司总是在修改硬件，并要求客户每次都重新编译他们的应用程序，维护一系列不同的二进制文件是不切实际的。

EPIC 通过添加一层抽象来解决这个问题，通常采用计算机科学的方法。而不是明确指定指令应在处理器的哪个部分执行，EPIC 创建了一个包含几个核心单元（如内存、整数和浮点数）的简化视图。

</main>

<main class="calibre3">

## 2 内存

### 2.1 内存层次结构

CPU 只能直接从位于处理器芯片上的缓存内存中获取指令和数据。缓存内存必须从主系统内存（随机存取存储器，或 RAM）中加载。然而，RAM 只有在电源开启时才保留其内容，因此需要存储在更持久的存储器上。

我们将这些内存层次称为 *内存层次结构*

表 2.1.1 内存层次结构

| 速度 | 内存 | 描述 |
| --- | --- | --- |
| 最快 | 缓存 | 缓存内存实际上是嵌入在 CPU 内部的内存。缓存内存非常快，通常只需一个周期即可访问，但由于它直接嵌入到 CPU 中，因此其大小有限。实际上，缓存内存有多个子级别（称为 L1、L2、L3），它们的速度略有增加。 |
|  | RAM | 处理器的所有指令和存储地址都必须来自 RAM。尽管 RAM 非常快，但 CPU 访问它仍需要一些显著的时间（这被称为 *延迟*）。RAM 存储在连接到主板的独立、专用芯片上，这意味着它比缓存内存大得多。 |
| 最慢 | 磁盘 | 我们都熟悉软件通过软盘或 CDROM 到来，以及将我们的文件保存到硬盘上。我们也熟悉程序从硬盘加载所需的时间很长——具有旋转磁盘和移动头等物理机制意味着磁盘是存储速度最慢的形式。但它们也是迄今为止最大的存储形式。 |

关于内存层次结构的重要一点是要了解速度和大小之间的权衡——内存越快，它就越小。当然，如果你能找到改变这个等式的方法，你最终会成为一个亿万富翁！

缓存之所以有效，是因为计算机代码通常表现出两种形式的局部性

1.  *空间* 局部性表明，块内的数据很可能一起被访问。

1.  *时间* 局部性表明，最近使用过的数据很可能会很快再次被使用。

这意味着，通过尽可能实现快速访问的内存（时间）存储尽可能小的相关信息块（空间），可以获得好处。

### 2.2 缓存深入

缓存是 CPU 架构中最重要的元素之一。为了编写高效的代码，开发者需要了解他们系统中的缓存是如何工作的。

缓存是较慢的主系统内存的非常快速的副本。由于它包含在处理器芯片上，与寄存器和处理器逻辑一起，因此缓存比主存储器小得多。这在计算术语中是黄金地段，其最大尺寸存在经济和物理限制。随着制造商找到越来越多在芯片上塞入更多晶体管的方法，缓存大小显著增长，但即使最大的缓存也只有几十兆字节，而不是主存储器的千兆字节或硬盘的兆字节。

缓存由主内存的小块镜像组成。这些块的大小称为*行大小*，通常是 32 或 64 字节。在谈论缓存时，通常谈论行大小或缓存行，它指的是主内存的一个块。缓存只能以缓存行大小的倍数加载和存储内存。

缓存有自己的层次结构，通常称为 L1、L2 和 L3。L1 缓存是最快和最小的；L2 更大但更慢，L3 则更慢。

L1 缓存通常进一步分为指令缓存和数据缓存，称为“哈佛架构”，这是基于继电器式哈佛 Mark-1 计算机引入的。分割缓存有助于减少流水线瓶颈，因为早期流水线阶段倾向于引用指令缓存，而后期阶段则引用数据缓存。除了减少对共享资源的竞争外，为指令提供单独的缓存还允许采用可能利用指令流性质的替代实现；它们是只读的，因此不需要昂贵的芯片功能，如多端口，也不需要处理子块读取，因为指令流通常使用更规则的访问大小。

![图片](img/sets.svg)

给定的缓存行可能在一个阴影条目中找到一个有效的家。

图 2.2.1 缓存关联性

在正常操作期间，处理器不断要求缓存检查特定地址是否存储在缓存中，因此缓存需要某种方法来非常快速地确定是否存在有效的行。如果给定地址可以在缓存中的任何位置进行缓存，每次引用时都需要搜索每个缓存行以确定命中或未命中。为了保持搜索速度快，这在缓存硬件中是并行进行的，但搜索每个条目通常对于合理大小的缓存来说成本太高，无法实现。因此，可以通过限制特定地址必须存在的位置来简化缓存。这是一个权衡；显然，缓存比系统内存小得多，所以一些地址必须*别名*其他地址。如果两个别名彼此的地址被不断更新，它们就被称为*争夺*缓存行。因此，我们可以将缓存分为三种一般类型，如图 2.2.1，缓存关联性 Cache Associativity 所示。

+   *直接映射*缓存将允许缓存行只存在于缓存中的一个单独条目中。这在硬件中实现起来最简单，但如图 2.2.1，缓存相联性所示，由于两个阴影地址必须共享相同的缓存行，因此没有避免别名的潜力。

+   *全相联*缓存将允许缓存行存在于缓存中的任何条目中。这避免了别名问题，因为任何条目都可以使用。但在硬件中实现起来非常昂贵，因为必须同时查找所有可能的位置以确定值是否在缓存中。

+   *组相联*缓存是直接相联缓存和全相联缓存的一种混合，允许特定的缓存值存在于缓存中某些行的子集中。缓存被划分为称为*路*的偶数部分，特定的地址可以位于任何一路。因此，一个*n*路组相联缓存将允许缓存行存在于大小为总块数模 n 的集合中的任何条目中——图 2.2.1，缓存相联性展示了一个样本 8 元素，4 路组相联缓存；在这种情况下，两个地址有四个可能的位置，这意味着在查找时只需搜索缓存的一半。路越多，可能的位置越多，别名越少，从而整体性能更好。

当缓存已满时，处理器需要移除一行以腾出空间给新行。有许多算法可以让处理器选择移除哪一行；例如，*最近最少使用*（LRU）是一种算法，其中最老的未使用行被丢弃以腾出空间给新行。

当数据只从缓存中读取时，没有必要确保与主内存的一致性。然而，当处理器开始写入缓存行时，它需要就如何更新底层主内存做出一些决定。*写通*缓存会在处理器更新缓存的同时直接将更改写入主系统内存。这比较慢，因为写入主内存的过程，正如我们所见，比较慢。或者，*写回*缓存会延迟将更改写入 RAM，直到绝对必要时才进行。明显的优点是，当缓存条目被写入时，所需的内存访问较少。已写入但尚未提交到内存的缓存行被称为*脏数据*。缺点是，当缓存条目被移除时，可能需要两次内存访问（一次写入脏数据到主内存，另一次加载新数据）。

如果一个条目同时存在于高级缓存和低级缓存中，我们说高级缓存是*包含的*。或者，如果高级缓存具有一行并消除了低级缓存具有该行的可能性，我们说它是*排除的*。这个选择在第 4.1.1.1 节，SMP 系统中的缓存排他性中进一步讨论。

#### 2.2.1 缓存寻址

到目前为止，我们还没有讨论缓存如何决定给定的地址是否位于缓存中。显然，缓存必须保留一个目录，记录当前哪些数据位于缓存行中。缓存目录和数据可以位于处理器上，也可以是分开的——例如，在 POWER5 处理器的情况下，它有一个核心 L3 目录，但实际访问数据需要遍历 L3 总线来访问离核内存。这样的安排可以加快命中/未命中的处理速度，而无需承担保持整个缓存在核心的其他成本。

<picture>![](img/tags.svg)</picture>

标签需要并行检查以保持低延迟时间；更多的标签位（即更少的组关联性）需要更复杂的硬件来实现这一点。或者，更多的组关联性意味着更少的标签，但处理器现在需要硬件来多路复用许多组的输出，这也可能增加延迟。

图 2.2.1.1 缓存标签

为了快速判断地址是否位于缓存中，它被分为三部分；*标签*、*索引*和*偏移量*。

偏移位数取决于缓存的行大小。例如，32 字节行大小将使用地址的最后 5 位（即 2⁵）作为行内的偏移量。

*索引*是条目可能驻留的特定缓存行。例如，让我们考虑一个有 256 个条目的缓存。如果这是一个直接映射缓存，我们知道数据可能只驻留在一条可能的行中，因此偏移量之后的下一个 8 位（2⁸）描述了要检查的行——介于 0 到 255 之间。

现在，考虑相同的 256 元素缓存，但将其分为两种方式。这意味着有两组 128 行，给定的地址可能位于这两组中的任意一组。因此，只需要 7 位作为索引来偏移到 128 个条目的方式中。对于给定的缓存大小，随着方式的增加，我们减少作为索引所需的位数，因为每个方式都变得更小。

缓存目录仍然需要检查缓存中存储的特定地址是否是它感兴趣的地址。因此，地址的剩余位是*标签*位，缓存目录将其与传入地址的标签位进行比较，以确定是否发生缓存命中。这种关系如图 2.2.1.1“缓存标签”所示。

当有多个方式时，这种检查必须在每个方式内并行进行，然后将其结果传递给一个多路复用器，输出最终的*命中*或*未命中*结果。如上所述，缓存越关联，所需的索引位数越少，标签位数越多——直到完全关联的缓存，其中不使用任何位作为索引位。标签位的并行匹配是缓存设计中的昂贵组件，通常也是缓存可能增长行数（即大小）的限制因素。

</main>

<main class="calibre3">

## 3 外设和总线

外设是连接到您计算机的许多外部设备之一。显然，处理器必须有一些与外围设备通信的方法，以便使它们有用。

处理器和外围设备之间的通信通道被称为**总线**。

### 3.1 外设总线概念

一个设备需要输入和输出才能有用。与外围设备进行有效通信需要一些常见概念。

#### 3.1.1 中断

中断允许设备实际上中断处理器以标记某些信息。例如，当按键被按下时，会生成一个中断将按键事件传递给操作系统。每个设备都由操作系统和 BIOS 的某种组合分配一个中断。

设备通常连接到一个**可编程中断控制器**（PIC），这是一个作为主板一部分的独立芯片，它缓冲并传递中断信息到主处理器。每个设备都与系统提供的 PIC 之一之间有一个物理的**中断线**。当设备想要中断时，它会修改这条线上的电压。

对 PIC 角色的非常广泛的描述是，它接收这个中断并将其转换为供主处理器消费的消息。虽然具体过程因架构而异，但一般原则是操作系统已配置了一个**中断描述符表**，将每个可能的中断与接收中断时要跳转的代码地址配对。这如图图 3.1.1.1，处理中断概述所示。

编写这个**中断处理程序**是设备驱动程序作者与操作系统共同的任务。

![中断](img/interrupt.svg)

处理中断的通用概述。设备将中断提升到中断控制器，控制器将信息传递给处理器。处理器查看由操作系统填充的描述符表，以找到处理故障的代码。

图 3.1.1.1 处理中断概述

大多数驱动程序会将中断处理分为**底部**和**顶部**两部分。底部部分将确认中断，排队处理动作，并快速将处理器返回到之前的状态。顶部部分将在 CPU 空闲时运行，执行更密集的处理。这是为了防止中断占用整个 CPU。

##### 3.1.1.1 保存状态

由于中断可能随时发生，因此当处理完中断后能够返回到运行的操作是很重要的。通常，这是操作系统的职责，确保在进入中断处理程序时保存任何**状态**；即寄存器，并在从中断处理程序返回时恢复它们。这样，除了丢失一些时间外，中断对当时正在运行的操作是完全透明的。

##### 3.1.1.2 中断与陷阱和异常

虽然中断通常与来自物理设备的外部事件相关联，但同样的机制也适用于处理内部系统操作。例如，如果处理器检测到诸如访问无效内存、尝试除以零或无效指令等条件，它可以内部提升一个*异常*由操作系统处理。这也是用于处理*系统调用*的机制，如第三部分，系统调用中所述，以及实现虚拟内存的机制，如第六章，虚拟内存中所述。尽管这些中断是由内部生成而不是外部源，但异步中断运行代码的原则仍然是相同的。

##### 3.1.1.3 中断类型

在线上触发中断主要有两种方式——*电平*触发和*边缘*触发。

电平触发中断定义了中断线保持高电平的电压，以指示有中断待处理。边缘触发中断检测总线上的*转换*；也就是说，当线路电压从低到高变化时。在边缘触发中断中，PIC 通过检测方波脉冲来识别中断已被提升。

当设备共享一个中断线时，差异尤为明显。在电平触发系统中，中断线将保持高电平，直到所有已引发中断的设备都被处理并取消其中断声明。

在边缘触发系统中，线路上的脉冲将向 PIC 指示已发生中断，PIC 将向操作系统发出信号以进行处理。然而，如果来自另一个设备的进一步脉冲出现在已经声明的中断线上。

电平触发中断的问题在于处理设备中断可能需要相当长的时间。在这段时间内，中断线保持高电平，无法确定是否有其他设备在该线上引发了中断。这意味着在服务中断时可能会有相当大的不可预测的延迟。

在边缘触发中断中，一个长时间运行的中断可以被注意到并排队，但其他共享该线的设备仍然可以转换（因此引发中断），而这一过程正在进行。然而，这也引入了新的问题；如果两个设备同时中断，可能会错过其中一个中断，或者环境或其他干扰可能会创建一个应该被忽略的*虚假*中断。

##### 3.1.1.4 不可屏蔽中断

对于系统来说，能够在某些时候*屏蔽*或防止中断是很重要的。通常情况下，可以将中断挂起，但有一类特殊的中断，称为*不可屏蔽中断*（NMI），是这一规则的例外。典型的例子是*复位*中断。

NMI 可以用于实现系统看门狗等功能，其中定期引发 NMI 并设置操作系统必须确认的一些标志。如果在下一个周期性 NMI 之前没有看到确认，则可以认为系统没有向前发展。另一种常见用途是分析系统。可以引发周期性 NMI 并用于评估处理器当前正在运行的代码；随着时间的推移，这将构建一个正在运行的代码配置文件，并为系统性能提供非常有用的见解。

#### 3.1.2 I/O 空间

显然，处理器需要与外围设备通信，它通过 I/O 操作来完成。最常见的 I/O 形式是所谓的*内存映射 I/O*，其中设备上的寄存器被*映射*到内存中。

这意味着要与设备通信，你只需简单地读取或写入内存中的特定地址。TODO：扩展

### 3.2 DMA

由于设备速度远低于处理器速度，需要有一种方法来避免 CPU 等待从设备获取数据。

直接内存访问（DMA）是在外围设备和系统 RAM 之间直接传输数据的方法。

驱动程序可以通过提供放置数据的 RAM 区域来设置设备进行 DMA 传输。然后它可以启动 DMA 传输，并允许 CPU 继续执行其他任务。

一旦设备完成，它将引发中断并通知驱动程序传输已完成。从这时起，设备的数据（例如来自磁盘的文件或视频捕获卡的帧）就在内存中，并准备好使用。

### 3.3 其他总线

其他总线连接在 PCI 总线和外部设备之间。

#### 3.3.1 USB

从操作系统的角度来看，一个 USB 设备是一组端点组合成一个接口。端点可以是*输入*或*输出*，因此只能单向传输数据。端点可以有几种不同的类型：

+   *控制*端点是用于配置设备等。

+   *中断*端点用于传输少量数据。它们的优先级高于...

+   *批量*端点，传输大量数据但不保证时间约束。

+   *等时*传输是高优先级的实时传输，但如果错过了，则不会重试。这是用于流式传输数据（如视频或音频）的情况，再次发送数据没有意义。

可以有多个接口（由多个端点组成），接口被分组到*配置*中。然而，大多数设备只有一个配置。

<picture>![](img/uhci.svg)</picture>

UCHI 控制器的概述，摘自[英特尔文档](http://download.intel.com/technology/usb/UHCI11D.pdf) (http://download.intel.com/technology/usb/UHCI11D.pdf)。

图 3.3.1.1 UHCI 控制器操作概述

图 3.3.1.1，UHCI 控制器操作概述显示了通用主机控制器接口，或 UHCI 的概述。它概述了 USB 数据如何通过硬件和软件的组合从系统中移出。本质上，软件为主控制器设置了一个特定格式的数据模板，以便读取并发送通过 USB 总线。

从概述的左上角开始，控制器有一个带有计数器的**帧**寄存器，该计数器会定期增加——每毫秒一次。这个值用于索引由软件创建的**帧列表**。表中每一项都指向一个**传输描述符**队列。软件在内存中设置这些数据，并由作为单独芯片的主控制器读取，该芯片驱动 USB 总线。软件需要调度工作队列，以便 90%的帧时间用于等时数据，10%留作中断、控制和批量数据。

如从图中所示，数据的链接方式意味着等时数据的传输描述符仅与一个特定的帧指针相关联——换句话说，仅与一个特定的时间段相关联——之后将被丢弃。然而，中断、控制和批量数据都是在等时数据之后**排队**的，因此如果在一个帧（时间段）内没有传输，将在下一个帧（时间段）内完成。

USB 层通过 USB**请求块**，或 URBs 进行通信。一个 URB 包含有关此请求相关的端点、数据、任何相关信息或属性以及当 URB 完成时要调用的回调函数的信息。USB 驱动程序以固定格式将 URBs 提交给 USB 核心，该核心与上述 USB 主机控制器协调管理它们。您的数据由 USB 核心发送到 USB 设备，完成后将触发回调。

</main>

<main class="calibre3">

## 4 从小型到大型系统

如摩尔定律所预测，计算能力一直在以惊人的速度增长，并且没有放缓的迹象。高端服务器仅包含单个 CPU 的情况相对较少。这是通过多种不同的方式实现的。

### 4.1 对称多处理

对称多处理，通常简称为**SMP**，是目前在单个系统中包含多个 CPU 的最常见配置。

对称一词指的是系统中的所有 CPU 都是相同的（例如，架构、时钟速度）。在对称多处理系统中，有多个处理器共享其他所有系统资源（内存、磁盘等）。

#### 4.1.1 缓存一致性

在大多数情况下，系统中的 CPU 独立工作；每个都有自己的寄存器、程序计数器等。尽管它们独立运行，但有一个组件需要严格的同步。

这就是 CPU 缓存；记住缓存是一个快速可访问的小区域内存，它反映了存储在主系统内存中的值。如果一个 CPU 修改了主内存中的数据，而另一个 CPU 在其缓存中有一个该内存的旧副本，系统显然不会处于一致状态。请注意，问题仅在处理器向内存写入时才会发生，因为如果只读取值，数据将是一致的。

为了在所有处理器上协调保持缓存一致性，SMP 系统使用**窥探**。窥探是指处理器监听所有处理器都连接到的总线上发生的缓存事件，并相应地更新其缓存。

用于此目的的一个协议是**MOESI**协议；代表修改、所有者、独占、共享、无效。这些是系统中的处理器上缓存行可以处于的状态。然而，还有其他协议可以完成同样的工作，但它们都共享类似的概念。以下我们将检查 MOESI，以便您了解这个过程涉及的内容。

当处理器需要从主内存读取缓存行时，它首先必须窥探系统中的所有其他处理器，以查看它们是否目前对该内存区域（例如，已将其缓存在缓存中）有任何了解。如果它不存在于任何其他进程中，那么处理器可以将内存加载到缓存中，并将其标记为**独占**。当它向缓存写入时，它随后将状态更改为**修改**。在这里，缓存的特定细节就发挥作用了；一些缓存会立即将修改后的缓存写回系统内存（称为**写通**缓存，因为写入会通过主内存）。而另一些则不会，并且仅在缓存中保留修改后的值，直到缓存被驱逐，例如当缓存变满时。

另一种情况是处理器进行窥探并发现该值在另一个处理器的缓存中。如果这个值已经被标记为**修改**，它将数据复制到自己的缓存中，并将其标记为**共享**。它将向另一个处理器（我们从那里获取数据）发送消息，将其缓存行标记为**所有者**。现在想象系统中的第三个处理器也想使用那个内存。它将进行窥探并发现一个**共享**和一个**所有者**副本；因此，它将从**所有者**值中获取其值。当所有其他处理器都在读取值时，缓存行在系统中保持**共享**。然而，当一个处理器需要更新值时，它将通过系统发送一个**无效**消息。任何具有该缓存行的处理器都必须将其标记为无效，因为它不再反映“真实”值。当处理器发送无效消息时，它在自己的缓存中将缓存行标记为**修改**，而所有其他处理器都会将其标记为**无效**（注意，如果缓存行是**独占**的，处理器知道没有其他处理器依赖于它，因此可以避免发送无效消息）。

从这个点开始，整个过程重新开始。因此，无论哪个处理器拥有*修改过的*值，都有责任在它从缓存中移除时将真实值写回 RAM。通过思考协议，你可以看到这确保了处理器之间缓存行的连续性。

随着处理器数量的增加，这个系统存在几个问题。只有少数处理器时，检查另一个处理器是否拥有缓存行（读取监视）或使每个其他处理器中的数据无效（无效监视）的开销是可管理的；但随着处理器数量的增加，总线流量也随之增加。这就是为什么 SMP 系统通常只能扩展到大约 8 个处理器。

将所有处理器都放在同一个总线上也开始出现物理问题。导线的物理特性只允许它们以一定的距离排列，并且只能有一定的长度。对于运行在许多吉赫兹的处理器来说，光速开始成为消息在系统中移动所需时间的实际考虑因素。

注意，系统软件通常不参与这个过程，尽管程序员应该意识到硬件在设计程序以最大化性能时在底层所做的事情。

##### 4.1.1.1 SMP 系统中的缓存排他性

在第 2.2 节，深入探讨缓存中，我们描述了*包含*和*排除*缓存。一般来说，L1 缓存通常是包含的——也就是说，L1 缓存中的所有数据也存在于 L2 缓存中。在多处理器系统中，包含的 L1 缓存意味着只需要 L2 缓存检查内存流量以保持一致性，因为 L2 中的任何变化都将保证在 L1 中得到反映。这降低了 L1 的复杂性，并将其与监视过程解耦，使其更快。

再次，一般来说，大多数现代高端（例如，不是针对嵌入式）处理器对 L1 缓存采用写通策略，而对低级缓存采用写回策略。这样做有几个原因。由于在这个处理器类别中，L2 缓存几乎全部位于芯片上，并且通常相当快，因此 L1 写通带来的惩罚并不是主要考虑因素。此外，由于 L1 的大小较小，未来不太可能被读取的写入数据池可能会污染有限的 L1 资源。另外，如果 L1 有未处理的脏数据，写通 L1 不必担心，因此可以将额外的保持一致性的逻辑传递给 L2（正如我们提到的，L2 在缓存一致性中已经扮演了更大的角色）。

#### 4.1.2 超线程

现代处理器的大部分时间都花在等待内存层次结构中速度较慢的设备提供数据处理所需的数据上。

因此，保持处理器流水线满载的策略至关重要。一种策略是包括足够的寄存器和状态逻辑，以便可以同时处理两个指令流。这使得一个 CPU 在所有意图和目的上看起来像两个 CPU。

虽然每个 CPU 都有自己的寄存器，但它们仍然必须共享核心逻辑、缓存以及从 CPU 到内存的输入和输出带宽。因此，尽管两个指令流可以使处理器核心更忙碌，但性能提升不会像有两个物理上独立的 CPU 那样大。通常，性能提升低于 20%（XXX 检查），然而，它可以根据工作负载显著提高或降低。

#### 4.1.3 多核

随着在芯片上放置更多和更多晶体管的能力增强，将两个或更多处理器放在同一个物理封装中成为可能。最常见的是双核，其中两个处理器核心位于同一芯片上。这些核心与超线程不同，它们是完整的处理器，因此看起来像 SMP 系统中的两个物理上独立的处理器。

虽然处理器通常有自己的 L1 缓存，但它们确实必须共享连接到主内存和其他设备的总线。因此，性能不如完整的 SMP 系统，但比超线程系统好得多（实际上，每个核心仍然可以实施超线程以实现额外的增强）。

多核处理器也有一些与性能无关的优势。正如我们提到的，处理器之间的外部物理总线有物理限制；通过将处理器放在同一块硅片上，彼此非常接近，可以解决这些问题中的某些问题。多核处理器的功耗远低于两个独立的处理器。这意味着需要散发的热量更少，这在数据中心应用中可以是一个很大的优势，因为计算机密集堆叠，冷却考虑因素可能很大。通过将核心放在同一个物理封装中，使得在笔记本电脑等应用中，它可以在其他情况下无法实现多处理。而且，只需要生产一个芯片而不是两个，这也大大降低了成本。

### 4.2 集群

许多应用需要比 SMP 系统可以扩展的处理器数量多得多的系统。进一步扩展系统的一种方法是使用*集群*。

集群简单来说就是一些能够相互通信的独立计算机。在硬件层面，系统之间没有相互了解；将独立计算机连接在一起的任务留给了软件。

如 MPI 之类的软件允许程序员编写他们的软件，然后将程序的一部分“外包”给系统中的其他计算机。例如，想象一个执行数千次独立操作的循环（即循环的每次迭代都不会影响其他迭代）。在一个由四台计算机组成的集群中，软件可以使每台计算机执行 250 次循环。

计算机之间的互连速度各不相同，可能慢到与互联网连接相当，也可能快到专用、特殊的总线（Infiniband）的速度。然而，无论哪种互连方式，它仍然位于内存层次结构的较低层，并且比 RAM 慢得多。因此，在需要每个 CPU 访问可能存储在另一台计算机 RAM 中的数据的情况下，集群的表现不会很好；因为每次发生这种情况时，软件都需要从另一台计算机请求数据的副本，通过慢速链路复制，并进入本地 RAM，然后处理器才能开始工作。

然而，许多应用*并不*需要这种在计算机之间不断复制数据。一个大规模的例子是 SETI@Home，其中从无线电天线收集的数据被分析以寻找外星生命的迹象。每台计算机可以分配几分钟的数据进行分析，并且只需要报告它发现了什么。SETI@Home 实际上是一个非常大的、专用的集群。

另一个应用是图像渲染，尤其是在电影中的特效。每台计算机可以处理电影的单帧，其中包含需要组合（渲染）成我们今天所看到的惊人特效的线框模型、纹理和光源。由于每一帧都是静态的，一旦计算机获得了初始输入，它就不需要更多的通信，直到最终帧准备好发送回并组合到电影中。例如，大片《指环王》的特效就是在运行 Linux 的巨大集群上渲染的。

### 4.3 非均匀内存访问

非均匀内存访问（Non-Uniform Memory Access），通常简称为 NUMA，几乎是上述集群系统的对立面。与集群系统一样，它由相互连接的独立节点组成，然而节点之间的连接非常专业（并且昂贵！）。与集群系统中的硬件对节点之间的连接一无所知不同，在 NUMA 系统中，*软件*对系统的布局和硬件的了解（好吧，较少）并且硬件负责将节点连接在一起的所有工作。

“非均匀内存访问”（non uniform memory access）这个术语来源于 RAM 可能不在 CPU 本地，因此可能需要从某个距离较远的节点访问数据。这显然需要更长的时间，与单处理器或 SMP 系统形成对比，在单处理器或 SMP 系统中，RAM 是直接连接的，并且总是需要恒定（均匀）的时间来访问。

#### 4.3.1 NUMA 机器布局

在一个系统中，有如此多的节点相互通信，最小化每个节点之间的距离至关重要。显然，如果每个节点都能直接连接到其他所有节点，这将最小化任何节点查找数据所需经过的距离。然而，当节点数量开始增长到数百甚至数千时，这种情况并不实际，就像在大型超级计算机中那样；如果你还记得你的高中数学，这个问题基本上是每次取两个的组合（每个节点与其他节点通信），并且会增长到 `n!/2*(n-2)!`。

为了对抗这种指数级增长，使用不同的布局来权衡节点之间的距离和所需的互连。在现代 NUMA 架构中常见的一种布局就是超立方体。

超立方体有一个严格的数学定义（远超出这次讨论的范围），但作为一个立方体是平方的三维对应物，所以超立方体是立方体的四维对应物。

<picture>![](img/hypercube.svg)</picture>

这是一个超立方的例子。超立方体在节点之间的距离和所需的互连数量之间提供了一个良好的折衷方案。

图 4.3.1.1 超立方体

在上面，我们可以看到外层立方体包含四个 8 节点的立方体。任何节点与其他节点通信所需的最大路径数是 3。当另一个立方体放置在这个立方体内部时，我们现在有双倍的处理器数量，但最大路径成本仅增加到 4。这意味着随着处理器数量以 2^n 的速度增长，最大路径成本仅线性增长。

#### 4.3.2 缓存一致性

在 NUMA 系统中，缓存一致性仍然可以保持（这被称为缓存一致性 NUMA 系统，或 ccNUMA）。正如我们提到的，用于在 SMP 系统中保持处理器缓存一致性的基于广播的方案无法扩展到大型 NUMA 系统中的数百甚至数千个处理器。在 NUMA 系统中用于缓存一致性的一个常见方案被称为 *基于目录的模型*。在这个模型中，系统中的处理器与特殊的缓存目录硬件通信。目录硬件为每个处理器维护一个一致的图像；这种抽象隐藏了 NUMA 系统的工作原理，使其对处理器不可见。

基于 Censier 和 Feautrier 目录的方案维护一个中央目录，其中每个内存块都有一个标志位，称为每个处理器的 *有效位* 和一个单独的 *脏位*。当一个处理器将内存读入其缓存时，目录会设置该处理器的有效位。

当一个处理器希望写入缓存行时，目录需要设置内存块的脏位。这涉及到向使用缓存行的那些处理器（以及只有那些标志位被设置的处理器）发送无效消息（避免广播流量）。

在此之后，如果任何其他处理器尝试读取内存块，目录将发现脏位被设置。目录将需要从当前设置有效位的处理器获取更新的缓存行，将脏数据写回主内存，然后向请求的处理器提供该数据，并在过程中为请求的处理器设置有效位。请注意，这对请求的处理器和目录来说是透明的，目录可能需要从非常近或非常远的地方获取这些数据。

显然，有数千个处理器与单个目录通信也并不具有良好的可扩展性。该方案的扩展包括具有目录层次结构，这些目录使用单独的协议相互通信。目录可以使用更通用的通信网络来相互通信，而不是 CPU 总线，从而允许扩展到更大的系统。

#### 4.3.3 NUMA 应用

NUMA 系统最适合需要处理器和内存之间大量交互的问题类型。例如，在天气模拟中，常见的做法是将环境分成小的“盒子”，它们以不同的方式响应（例如，海洋和陆地反射或存储不同数量的热量）。随着模拟的进行，将输入小的变化以查看整体结果。由于每个盒子都会影响周围的盒子（例如，多一点阳光意味着特定的盒子会释放更多的热量，影响相邻的盒子），因此会有大量的通信（与渲染过程中的单个图像帧进行对比，每个图像帧都不会影响其他图像帧）。如果你在模拟汽车碰撞，每个模拟汽车的小盒子以某种方式折叠并吸收一定量的能量，也可能发生类似的过程。

尽管软件没有直接了解底层系统是 NUMA 系统，程序员在编写系统代码以获得最佳性能时需要小心。显然，将内存保持在将要使用它的处理器附近将导致最佳性能。程序员需要使用诸如*性能分析*等技术来分析代码路径以及他们的代码对系统造成的影响，以提取最佳性能。

### 4.4 内存排序、锁定和原子操作

多级缓存、超标量多处理器架构带来了与程序员如何看待处理器运行代码相关的一些有趣问题。

假想程序代码同时在两个处理器上运行，两个处理器实际上共享一个大型内存区域。如果一个处理器发出存储指令，将寄存器值放入内存，它何时可以确信另一个处理器在读取该内存时将看到正确的值？

在最简单的情况下，系统可以保证如果程序执行了一个存储指令，任何后续的加载指令都将看到这个值。这被称为*严格的内存排序*，因为规则不允许有任何移动的空间。你应该开始意识到为什么这种事情会对系统的性能造成严重的阻碍。

大多数时候，内存排序不需要如此严格。程序员可以确定需要确保所有未完成的操作都全局可见的点，但在这些点之间，可能有许多指令的语义并不重要。

以以下情况为例。

```cpp
 1 |typedef struct { 
 |int a; 
 |int b; 
 |} a_struct; 
 5 |
 |/* 
 | * Pass in a pointer to be allocated as a new structure 
 | */ 
 |void get_struct(a_struct *new_struct) 
10 |{ 
 | void *p = malloc(sizeof(a_struct)); 
 |
 | /* We don't particularly care what order the following two 
 | * instructions end up acutally executing in */ 
15 | p->a = 100; 
 | p->b = 150; 
 |
 | /* However, they must be done before this instruction. 
 | * Otherwise, another processor who looks at the value of p 
20 | * could find it pointing into a structure whose values have 
 | * not been filled out. 
 | */ 
 | new_struct = p; 
 |} 

```

示例 4.4.1 内存排序

在这个例子中，我们有两个可以以任何特定顺序执行的存储，因为这对处理器来说合适。然而，在最终情况下，指针必须在两个之前的存储已知已执行后才能更新。否则，另一个处理器可能会查看`p`的值，跟随指针到内存中，加载它，并得到一些完全错误的价值！

为了表示这一点，加载和存储必须具有*语义*来描述它们必须具有的行为。内存语义是用*栅栏*来描述的，这些栅栏规定了加载和存储可以在加载或存储周围如何重排序。

默认情况下，加载或存储可以在任何地方重排序。

*获取*语义就像一个只允许加载和存储向下通过它的栅栏。也就是说，当这个加载或存储完成时，你可以保证任何后续的加载或存储都将看到这个值（因为它们不能被移动到它上面）。

*释放*语义相反，即一个允许在它之前进行任何加载或存储的栅栏（向上移动），但没有任何东西在它之前向下移动过它。因此，当处理具有释放语义的加载或存储时，你可以确信任何更早的加载或存储都已经完成。

![内存排序](img/memorder.svg)

在具有获取和释放语义的操作周围的合法重排序的示例。

图 4.4.1 获取和释放语义

*完整的内存栅栏*是两者的结合；在当前加载或存储周围，任何加载或存储都不能在任何方向上重排序。

严格的内存模型会在每个操作中使用完整的内存栅栏。最弱的模型会将每个加载和存储操作都视为一个普通的可重排序指令。

#### 4.4.1 处理器和内存模型

不同的处理器实现不同的内存模型。

x86（和 AMD64）处理器有一个相当严格的内存模型；所有存储都有释放语义（即存储的结果保证被任何后续的加载或存储看到），但所有加载都有正常语义。lock 前缀提供内存栅栏。

Itanium 允许所有加载和存储都是正常的，除非明确告知。XXX

#### 4.4.2 锁定

了解每个架构的内存排序要求对于所有程序员来说并不实用，并且会使程序难以在不同处理器类型之间移植和调试。

程序员使用称为**锁定**的高级抽象级别，以便在存在多个 CPU 时允许程序同时运行。

当一个程序在一段代码上获得锁时，其他处理器在释放锁之前无法获得该锁。在执行任何关键代码之前，处理器必须尝试获取锁；如果无法获取，则不会继续执行。

你可以看到这与上一节中内存排序语义的命名是如何联系在一起的。我们希望在获取锁之前，不应该被锁保护的任何操作在它之前被重新排序。这就是获取语义的工作方式。

相反，当我们**释放**锁时，我们必须确保在我们持有锁期间所进行的每个操作都已经完成（还记得之前更新指针的例子吗？）。这就是释放语义。

有许多软件库可供程序员使用，允许他们不必担心内存语义的细节，而只需使用更高层次的抽象`lock()`和`unlock()`。

##### 4.4.2.1 锁定困难

锁定方案使得编程变得更加复杂，因为程序可能会发生**死锁**。想象一下，如果一个处理器目前正持有某些数据的锁，并且正在等待获取其他数据的锁。如果那个其他处理器在解锁第二个锁之前等待获取第一个处理器持有的锁，我们就遇到了死锁情况。每个处理器都在等待另一个处理器，并且没有其他处理器可以不获得对方的锁而继续执行。

这种情况通常是由于微妙的**竞态条件**引起的；这是最难追踪的 bug 之一。如果两个处理器依赖于在特定时间顺序中发生的操作，那么总有可能发生竞态条件。来自不同星系中爆炸恒星的高能伽马射线可能会击中其中一个处理器，使其跳过一个节拍，从而打乱操作的顺序。通常会发生的情况是像上面那样的死锁情况。正因为如此，程序顺序需要通过语义来确保，而不是依赖于特定时间的行为。

类似的情况是死锁的对立面，称为**活锁**。避免死锁的一种策略可能是拥有一个“礼貌”的锁；一个你可以给予任何请求者的锁。这种礼貌可能会导致两个线程不断地互相给予锁，但没有任何一个线程能够长时间持有锁来完成关键工作并结束锁（现实生活中类似的情况可能是两个人同时到达一扇门前，都坚持说“不，你先，我坚持”。结果两个人都没有通过门！）。

##### 4.4.2.2 锁定策略

在底层，有许多不同的策略来实现锁的行为。

一个简单的锁，只有两种状态——锁定或解锁，被称为**互斥锁**（缩写为互斥；也就是说，如果一个人有它，另一个人就不能有它）。

然而，有几种方法可以实现互斥锁。在最简单的情况下，我们有什么通常称为**自旋锁**。使用这种类型的锁，处理器会坐在一个紧密的循环中等待获取锁；相当于它不断地说“我现在可以吗”，就像一个小孩可能会向父母询问一样。

这种策略的问题在于它本质上浪费了时间。当处理器不断坐着请求锁时，它并没有做任何有用的工作。对于可能只短暂持有锁的锁，这可能合适，但在许多情况下，锁被持有的时间可能会更长。

因此，另一种策略是在锁上**睡眠**。在这种情况下，如果处理器不能获得锁，它将开始做一些其他工作，等待通知锁可用（我们将在未来的章节中看到操作系统如何切换进程并给处理器更多的工作做）。

然而，互斥锁（mutex）实际上只是**信号量**的一个特例，由荷兰计算机科学家迪杰斯特拉（Dijkstra）著名地发明。在存在多个资源可用的情况下，信号量可以被设置为计数对资源的访问次数。在资源数量为一种情况下，你就有了一个互斥锁。信号量的操作可以在任何算法书中详细说明。

然而，这些锁定方案仍然存在一些问题。在许多情况下，大多数人只想读取很少更新的数据。所有处理器都只想读取数据并需要获取锁可能会导致**锁竞争**，因为工作做得更少，因为每个人都正在等待获取相同数据的同一把锁。

#### 4.4.3 原子操作

解释一下它是什么。

</main>
