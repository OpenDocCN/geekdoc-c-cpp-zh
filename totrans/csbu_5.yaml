- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6. Virtual Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 1 What Virtual Memory *isn't*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Virtual memory is often naively discussed as a way to extended your RAM by using
    the hard drive as extra, slower, system memory. That is, once your system runs
    out of memory, it flows over onto the hard drive which is used as "virtual" memory.
  prefs: []
  type: TYPE_NORMAL
- en: In modern operating systems, this is commonly referred to as *swap space*, because
    unused parts of memory as swapped out to disk to free up main memory (remember,
    programs can only execute from main memory).
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the ability to swap out memory to disk is an important capability, but
    as you will see it is not the purpose of virtual memory, but rather a very useful
    side effect!
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 2 What virtual memory *is*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Virtual memory is all about making use of *address space*.
  prefs: []
  type: TYPE_NORMAL
- en: The address space of a processor refers the range of possible addresses that
    it can use when loading and storing to memory. The address space is limited by
    the width of the registers, since as we know to load an address we need to issue
    a `load` instruction with the address to load from stored in a register. For example,
    registers that are 32 bits wide can hold addresses in a register range from `0x00000000`
    to `0xFFFFFFF`. 2^^(32) is equal to 4GB, so a 32 bit processor can load or store
    to up to 4GB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 64 bit computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'New processors are generally all 64-bit processors, which as the name suggests
    has registers 64 bits wide. As an exercise, you should work out the address space
    available to these processors (hint: it is big!).'
  prefs: []
  type: TYPE_NORMAL
- en: 64-bit computing does have some trade-offs against using smaller bit-width processors.
    Every program compiled in 64-bit mode requires 8-byte pointers, which can increase
    code and data size, and hence impact both instruction and data cache performance.
    However, 64-bit processors tend to have more registers, which means less need
    to save temporary variables to memory when the compiler is under register pressure.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Canonical Addresses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While 64-bit processors have 64-bit wide registers, systems generally do not
    implement all 64-bits for addressing — it is not actually possible to do `load`
    or `store` to all 16 exabytes of theoretical physical memory!
  prefs: []
  type: TYPE_NORMAL
- en: Thus most architectures define an *unimplemented* region of the address space
    which the processor will consider invalid for use. x86-64 and Itanium both define
    the most-significant valid bit of an address, which must then be sign-extended
    (see [Section 2.3.1.3.1, Sign-extension](csbu-print_split_009.html#sign_extension))
    to create a valid address. The result of this is that the total address space
    is effectively divided into two parts, an upper and a lower portion, with the
    addresses in-between considered invalid. This is illustrated in [Figure 2.1.1.1,
    Illustration of canonical addresses](#canonical_address). Valid addresses are
    termed *canonical addresses* (invalid addresses being *non*-canonical).
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![By defining a most-significant bit that must be sign-extended to
    create a full address, the address-space is effectively partitioned into upper
    and lower portions, with intermediate addresses considered invalid by the processor.](canonical.svg)</picture>Figure 2.1.1.1 Illustration
    of canonical addresses
  prefs: []
  type: TYPE_NORMAL
- en: The exact most-significant bit value for the processor can usually be found
    by querying the processor itself using its informational instructions. Although
    the exact value is implementation dependent, a typical value would be 48; providing
    2^(48) = 256 TiB of usable address-space.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the possible address-space like this means that significant savings
    can be made with all parts of the addressing logic in the processor and related
    components, as they know they will not need to deal with full 64-bit addresses.
    Since the implementation defines the upper-bits as being signed-extended, this
    prevents portable operating systems using these bits to store or flag additional
    information and ensuring compatibility if the implementation wishes to implement
    more address-space in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Using the address space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with most components of the operating system, virtual memory acts as an abstraction
    between the address space and the physical memory available in the system. This
    means that when a program uses an address that address does not refer to the bits
    in an actual physical location in memory.
  prefs: []
  type: TYPE_NORMAL
- en: So to this end, we say that all addresses a program uses are *virtual*. The
    operating system keeps track of virtual addresses and how they are allocated to
    *physical* addresses. When a program does a load or store from an address, the
    processor and operating system work together to convert this virtual address to
    the actual address in the system memory chips.
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 3 Pages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The total address-space is divided into individual *pages*. Pages can be many
    different sizes; generally they are around 4 KiB, but this is not a hard and fast
    rule and they can be much larger but generally not any smaller. The page is the
    smallest unit of memory that the operating system and hardware can deal with.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, each page has a number of attributes set by the operating system.
    Generally, these include read, write and execute permissions for the current page.
    For example, the operating system can generally mark the code pages of a process
    with an executable flag and the processor can choose to not execute any code from
    pages without this bit set.
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![Pages](page.svg)</picture>Figure 3.1 Virtual memory pages
  prefs: []
  type: TYPE_NORMAL
- en: Programmers may at this point be thinking that they can easily allocate small
    amounts of memory, much smaller than 4 KiB, using `malloc` or similar calls. This
    *heap* memory is actually backed by page-size allocations, which the `malloc`
    implementation divides up and manages for you in an efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 4 Physical Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as the operating system divides the possible address space up into pages,
    it divides the available physical memory up into *frames*. A frame is just the
    conventional name for a hunk of physical memory the same size as the system page
    size.
  prefs: []
  type: TYPE_NORMAL
- en: The operating system keeps a *frame-table* which is a list of all possible pages
    of physical memory and if they are free (available for allocation) or not. When
    memory is allocated to a process, it is marked as used in the frame-table. In
    this way, the operating-system keeps track of all memory allocations.
  prefs: []
  type: TYPE_NORMAL
- en: How does the operating system know what memory is available? This information
    about where memory is located, how much, attributes and so forth is passed to
    the operating system by the BIOS during initialisation.
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 5 Pages + Frames = Page Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is the job of the operating system is to keep track of which of virtual-page
    points to which physical frame. This information is kept in a *page-table* which,
    in its simplest form, could simply be a table where each row contains its associated
    frame — this is termed a *linear page-table*. If you were to use this simple system,
    with a 32 bit address-space and 4 KiB pages there would be 1048576 possible pages
    to keep track of in the page table (2^(32) ÷ 4096); hence the table would be 1048576
    entries long to ensure we can always map a virtual page to a physical page.
  prefs: []
  type: TYPE_NORMAL
- en: Page tables can have many different structures and are highly optimised, as
    the process of finding a page in the page table can be a lengthy process. We will
    examine page-tables in more depth later.
  prefs: []
  type: TYPE_NORMAL
- en: The page-table for a process is under the exclusive control of the operating
    system. When a process requests memory, the operating system finds it a free page
    of physical memory and records the virtual-to-physical translation in the processes
    page-table. Conversely, when the process gives up memory, the virtual-to-physical
    record is removed and the underlying frame becomes free for allocation to another
    process.
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 6 Virtual Addresses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a program accesses memory, it does not know or care where the physical
    memory backing the address is stored. It knows it is up to the operating system
    and hardware to work together to map locate the right physical address and thus
    provide access to the data it wants. Thus we term the address a program is using
    to access memory a *virtual address*. A virtual address consists of two parts;
    the page and an offset into that page.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the entire possible address space is divided up into regular sized pages,
    every possible address resides within a page. The page component of the virtual
    address acts as an index into the page table. Since the page is the smallest unit
    of memory allocation within the system there is a trade-off between making pages
    very small, and thus having very many pages for the operating-system to manage,
    and making pages larger but potentially wasting memory
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Offset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last bits of the virtual address are called the *offset* which is the location
    difference between the byte address you want and the start of the page. You require
    enough bits in the offset to be able to get to any byte in the page. For a 4K
    page you require (4K == (4 * 1024) == 4096 == 2^(12) ==) 12 bits of offset. Remember
    that the smallest amount of memory that the operating system or hardware deals
    with is a page, so each of these 4096 bytes reside within a single page and are
    dealt with as "one".
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Virtual Address Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Virtual address translation refers to the process of finding out which physical
    page maps to which virtual page.
  prefs: []
  type: TYPE_NORMAL
- en: When translating a virtual-address to a physical-address we only deal with the
    *page number* . The essence of the procedure is to take the page number of the
    given address and look it up in the *page-table* to find a pointer to a physical
    address, to which the offset from the virtual address is added, giving the actual
    location in system memory.
  prefs: []
  type: TYPE_NORMAL
- en: Since the page-tables are under the control of the operating system, if the
    virtual-address doesn't exist in the page-table then the operating-system knows
    the process is trying to access memory that has not been allocated to it and the
    access will not be allowed.
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![Converting a virtual address to a physical address](virtaddress.svg)</picture>Figure 6.3.1 Virtual
    Address Translation
  prefs: []
  type: TYPE_NORMAL
- en: We can follow this through for our previous example of a simple *linear* page-table.
    We calculated that a 32-bit address-space would require a table of 1048576 entries
    when using 4KiB pages. Thus to map a theoretical address of 0x80001234, the first
    step would be to remove the offset bits. In this case, with 4KiB pages, we know
    we have 12-bits (2^(12) == 4096) of offset. So we would right-shift out 12-bits
    of the virtual address, leaving us with 0x80001\. Thus (in decimal) the value
    in row 524289 of the linear page table would be the physical frame corresponding
    to this page.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might see a problem with a linear page-table: since every page must be
    accounted for, whether in use or not, a physically linear page-table is completely
    impractical with a 64-bit address space. Consider a 64-bit address space divided
    into 64 KiB pages creates 2^(64)/2^(16) = 2^(52) pages to be managed; assuming
    each page requires an 8-byte pointer to a physical location a total of 2^(52)*2³
    = 2^(55) or 32 PiB of contiguous memory would be required just for the page table!
    There are ways to split addressing up that avoid this which we will discuss later.'
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 7 Consequences of virtual addresses, pages and page tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Virtual addressing, pages and page-tables are the basis of every modern operating
    system. It under-pins most of the things we use our systems for.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Individual address spaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By giving each process its own page table, every process can pretend that it
    has access to the entire address space available from the processor. It doesn't
    matter that two processes might use the same address, since different page-tables
    for each process will map it to a different frame of physical memory. Every modern
    operating system provides each process with its own address space like this.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, physical memory becomes *fragmented*, meaning that there are "holes"
    of free space in the physical memory. Having to work around these holes would
    be at best annoying and would become a serious limit to programmers. For example,
    if you `malloc` 8 KiB of memory; requiring the backing of two 4 KiB frames, it
    would be a huge unconvinced if those frames had to be contiguous (i.e., physically
    next to each other). Using virtual-addresses it does not matter; as far as the
    process is concerned it has 8 KiB of contiguous memory, even if those pages are
    backed by frames very far apart. By assigning a virtual address space to each
    process the programmer can leave working around fragmentation up to the operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Protection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We previously mentioned that the virtual mode of the 386 processor is called
    protected mode, and this name arises from the protection that virtual memory can
    offer to processes running on it.
  prefs: []
  type: TYPE_NORMAL
- en: In a system without virtual memory, every process has complete access to all
    of system memory. This means that there is nothing stopping one process from overwriting
    another processes memory, causing it to crash (or perhaps worse, return incorrect
    values, especially if that program is managing your bank account!)
  prefs: []
  type: TYPE_NORMAL
- en: This level of protection is provided because the operating system is now the
    layer of abstraction between the process and memory access. If a process gives
    a virtual address that is not covered by its page-table, then the operating system
    knows that that process is doing something wrong and can inform the process it
    has stepped out of its bounds.
  prefs: []
  type: TYPE_NORMAL
- en: Since each page has extra attributes, a page can be set read only, write only
    or have any number of other interesting properties. When the process tries to
    access the page, the operating system can check if it has sufficient permissions
    and stop it if it does not (writing to a read only page, for example).
  prefs: []
  type: TYPE_NORMAL
- en: Systems that use virtual memory are inherently more stable because, assuming
    the perfect operating system, a process can only crash itself and not the entire
    system (of course, humans write operating systems and we inevitably overlook bugs
    that can still cause entire systems to crash).
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Swap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also now see how the swap memory is implemented. If instead of pointing
    to an area of system memory the page pointer can be changed to point to a location
    on a disk.
  prefs: []
  type: TYPE_NORMAL
- en: When this page is referenced, the operating system needs to move it from the
    disk back into system memory (remember, program code can only execute from system
    memory). If system memory is full, then *another* page needs to be kicked out
    of system memory and put into the swap disk before the required page can be put
    in memory. If another process wants that page that was just kicked out back again,
    the process repeats.
  prefs: []
  type: TYPE_NORMAL
- en: This can be a major issue for swap memory. Loading from the hard disk is very
    slow (compared to operations done in memory) and most people will be familiar
    with sitting in front of the computer whilst the hard disk churns and churns whilst
    the system remains unresponsive.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 mmap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A different but related process is the memory map, or `mmap` (from the system
    call name). If instead of the page table pointing to physical memory or swap the
    page table points to a file, on disk, we say the file is `mmap`ed.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, you need to `open` a file on disk to obtain a file descriptor, and
    then `read` and `write` it in a sequential form. When a file is mmaped it can
    be accessed just like system RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Sharing memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Usually, each process gets its own page table, so any address it uses is mapped
    to a unique frame in physical memory. But what if the operating system points
    two page table-entries to the same frame? This means that this frame will be shared;
    and any changes that one process makes will be visible to the other.
  prefs: []
  type: TYPE_NORMAL
- en: You can see now how threads are implemented. In [Section 4.3.1, `clone`](csbu-print_split_024.html#linux_clone)
    we said that the Linux `clone()` function could share as much or as little of
    a new process with the old process as it required. If a process calls `clone()`
    to create a new process, but requests that the two processes share the same page
    table, then you effectively have a *thread* as both processes see the same underlying
    physical memory.
  prefs: []
  type: TYPE_NORMAL
- en: You can also see now how copy on write is done. If you set the permissions of
    a page to be read-only, when a process tries to write to the page the operating
    system will be notified. If it knows that this page is a copy-on-write page, then
    it needs to make a new copy of the page in system memory and point the page in
    the page table to this new page. This can then have its attributes updated to
    have write permissions and the process has its own unique copy of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Disk Cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a modern system, it is often the case that rather than having too little
    memory and having to swap memory out, there is more memory available than the
    system is currently using.
  prefs: []
  type: TYPE_NORMAL
- en: The memory hierarchy tells us that disk access is much slower than memory access,
    so it makes sense to move as much data from disk into system memory if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Linux, and many other systems, will copy data from files on disk into memory
    when they are used. Even if a program only initially requests a small part of
    the file, it is highly likely that as it continues processing it will want to
    access the rest of file. When the operating system has to read or write to a file,
    it first checks if the file is in its memory cache.
  prefs: []
  type: TYPE_NORMAL
- en: These pages should be the first to be removed as memory pressure in the system
    increases.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1 Page Cache
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A term you might hear when discussing the kernel is the *page cache*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *page cache* refers to a list of pages the kernel keeps that refer to files
    on disk. From above, swap page, mmaped pages and disk cache pages all fall into
    this category. The kernel keeps this list because it needs to be able to look
    them up quickly in response to read and write requests XXX: this bit doesn''t
    file?'
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 8 Hardware Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have only mentioned that hardware works with the operating system
    to implement virtual memory. However we have glossed over the details of exactly
    how this happens.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory is necessarily quite dependent on the hardware architecture,
    and each architecture has its own subtleties. However, there are are a few universal
    elements to virtual memory in hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Physical v Virtual Mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All processors have some concept of either operating in *physical* or *virtual*
    mode. In physical mode, the hardware expects that any address will refer to an
    address in actual system memory. In virtual mode, the hardware knows that addresses
    will need to be translated to find their physical address.
  prefs: []
  type: TYPE_NORMAL
- en: In many processors, this two modes are simply referred to as physical and virtual
    mode. Itanium is one such example. The most common processor, the x86, has a lot
    of baggage from days before virtual memory and so the two modes are referred to
    as *real* and *protected* mode. The first processor to implement protected mode
    was the 386, and even the most modern processors in the x86 family line can still
    do real mode, though it is not used. In real mode the processor implements a form
    of memory organisation called segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Issues with segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Segmentation is really only interesting as a historical note, since virtual
    memory has made it less relevant. Segmentation has a number of drawbacks, not
    the least of which it is very confusing for inexperienced programmers, which virtual
    memory systems were largely invented to get around.
  prefs: []
  type: TYPE_NORMAL
- en: In segmentation there are a number of registers which hold an address that is
    the start of a segment. The only way to get to an address in memory is to specify
    it as an offset from one of these segment registers. The size of the segment (and
    hence the maximum offset you can specify) is determined by the number of bits
    available to offset from segment base register. In the x86, the maximum offset
    is 16 bits, or only 64KImagine that the maximum offset was 32 bits; in this case
    the entire address space could be accessed as an offset from a segment at `0x00000000`
    and you would essentially have a flat layout -- but it still isn't as good as
    virtual memory as you will see. In fact, the only reason it is 16 bits is because
    the original Intel processors were limited to this, and the chips maintain backwards
    compatibility. . This causes all sorts of havoc if one wants to use an address
    that is more than 64K away, which as memory grew into the megabytes (and now gigabytes)
    became more than a slight inconvenience to a complete failure.
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![A segmentation problem. You only have three segment registers, and
    can only offset a short distance from each segment. How do you get to another
    address? You need to manually reorganise the segment registers, which quickly
    becomes a bottleneck.](segmentation.svg)</picture>Figure 8.1.1.1 Segmentation
  prefs: []
  type: TYPE_NORMAL
- en: In the above figure, there are three segment registers which are all pointing
    to segments. The maximum offset (constrained by the number of bits available)
    is shown by shading. If the program wants an address outside this range, the segment
    registers must be reconfigured. This quickly becomes a major annoyance. Virtual
    memory, on the other hand, allows the program to specify any address and the operating
    system and hardware do the hard work of translating to a physical address.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 The TLB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *Translation Lookaside Buffer* (or TLB for short) is the main component
    of the processor responsible for virtual-memory. It is a cache of virtual-page
    to physical-frame translations inside the processor. The operating system and
    hardware work together to manage the TLB as the system runs.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Page Faults
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When a virtual address is requested of the hardware — say via a `load` instruction
    requesting to get some data — the processor looks for the virtual-address to physical-address
    translation in its TLB. If it has a valid translation it can then combine this
    with the offset portion to go straight to the physical address and complete the
    load.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the processor can *not* find a translation in the TLB, the processor
    must raise a *page fault*. This is similar to an interrupt (as discussed before)
    which the operating system must handle.
  prefs: []
  type: TYPE_NORMAL
- en: When the operating system gets a page fault, it needs to go through its page-table
    to find the correct translation and insert it into the TLB.
  prefs: []
  type: TYPE_NORMAL
- en: In the case that the operating system can not find a translation in the page
    table, or alternatively if the operating system checks the permissions of the
    page in question and the process is not authorised to access it, the operating
    system must kill the process. If you have ever seen a segmentation fault (or a
    segfault) this is the operating system killing a process that has overstepped
    its bounds.
  prefs: []
  type: TYPE_NORMAL
- en: Should the translation be found, and the TLB currently be full, then one translation
    needs to be removed before another can be inserted. It does not make sense to
    remove a translation that is likely to be used in the future, as you will incur
    the cost of finding the entry in the page-tables all over again. TLBs usually
    use something like a *Least Recently Used* or LRU algorithm, where the oldest
    translation that has not been used is ejected in favour of the new one.
  prefs: []
  type: TYPE_NORMAL
- en: The access can then be tried again, and, all going well, should be found in
    the TLB and translated correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1.1 Finding the page table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When we say that the operating system finds the translation in the page table,
    it is logical to ask how the operating system finds the memory that has the page
    table.
  prefs: []
  type: TYPE_NORMAL
- en: The base of the page table will be kept in a register associated with each process.
    This is usually called the page-table base-register or similar. By taking the
    address in this register and adding the page number to it, the correct entry can
    be located.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Other page related faults
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are two other important faults that the TLB can generally generate which
    help to mange accessed and dirty pages. Each page generally contains an attribute
    in the form of a single bit which flags if the page has been accessed or is dirty.
  prefs: []
  type: TYPE_NORMAL
- en: An accessed page is simply any page that has been accessed. When a page translation
    is initially loaded into the TLB the page can be marked as having been accessed
    (else why were you loading it in?Actually, if you were loading it in without a
    pending access this would be called *speculation*, which is where you do something
    with the expectation that it will pay off. For example, if code was reading along
    memory linearly putting the next page translation in the TLB might save time and
    give a performance improvement.)
  prefs: []
  type: TYPE_NORMAL
- en: The operating system can periodically go through *all* the pages and clear the
    accessed bit to get an idea of what pages are currently in use. When system memory
    becomes full and it comes time for the operating system to choose pages to be
    swapped out to disk, obviously those pages whose accessed bit has not been reset
    are the best candidates for removal, because they have not been used the longest.
  prefs: []
  type: TYPE_NORMAL
- en: A dirty page is one that has data written to it, and so does not match any data
    already on disk. For example, if a page is loaded in from swap and then written
    to by a process, before it can be moved out of swap it needs to have its on disk
    copy updated. A page that is clean has had no changes, so we do not need the overhead
    of copying the page back to disk.
  prefs: []
  type: TYPE_NORMAL
- en: Both are similar in that they help the operating system to manage pages. The
    general concept is that a page has two extra bits; the dirty bit and the accessed
    bit. When the page is put into the TLB, these bits are set to indicate that the
    CPU should raise a fault .
  prefs: []
  type: TYPE_NORMAL
- en: When a process tries to reference memory, the hardware does the usual translation
    process. However, it also does an extra check to see if the accessed flag is *not*
    set. If so, it raises a fault to the operating system, which should set the bit
    and allow the process to continue. Similarly if the hardware detects that it is
    writing to a page that does not have the dirty bit set, it will raise a fault
    for the operating system to mark the page as dirty.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 TLB Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can say that the TLB used by the hardware but managed by software. It is
    up to the operating system to load the TLB with correct entries and remove old
    entries.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Flushing the TLB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The process of removing entries from the TLB is called *flushing*. Updating
    the TLB is a crucial part of maintaining separate address spaces for processes;
    since each process can be using the same virtual address not updating the TLB
    would mean a process might end up overwriting another processes memory (conversely,
    in the case of *threads* sharing the address-space is what you want, thus the
    TLB is *not* flushed when switching between threads in the same process).
  prefs: []
  type: TYPE_NORMAL
- en: On some processors, every time there is a context switch the entire TLB is flushed.
    This can be quite expensive, since this means the new process will have to go
    through the whole process of taking a page fault, finding the page in the page
    tables and inserting the translation.
  prefs: []
  type: TYPE_NORMAL
- en: Other processors implement an extra *address space ID* (ASID) which is added
    to each TLB translation to make it unique. This means each address space (usually
    each process, but remember threads want to share the same address space) gets
    its own ID which is stored along with any translations in the TLB. Thus on a context
    switch the TLB does *not* need to be flushed, since the next process will have
    a different address space ID and even if it asks for the same virtual address,
    the address space ID will differ and so the translation to physical page will
    be different. This scheme reduces flushing and increases overall system performance,
    but requires more TLB hardware to hold the ASID bits.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, this is implemented by having an additional register as part of the
    process state that includes the ASID. When performing a virtual-to-physical translation,
    the TLB consults this register and will only match those entries that have the
    same ASID as the currently running process. Of course the width of this register
    determines the number of ASID's available and thus has performance implications.
    For an example of ASID's in a processor architecture see [Section 10.2.1, Address
    spaces](csbu-print_split_039.html#itanium_address_spaces).
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Hardware v Software loaded TLB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the control of what ends up in the TLB is the domain of the operating
    system; it is not the whole story. The process described in [Section 8.2.1, Page
    Faults](#page_faults) describes a page-fault being raised to the operating system,
    which traverses the page-table to find the virtual-to-physical translation and
    installs it in the TLB. This would be termed a *software-loaded TLB* — but there
    is another alternative; the *hardware-loaded TLB*.
  prefs: []
  type: TYPE_NORMAL
- en: In a hardware loaded TLB, the processor architecture defines a particular layout
    of page-table information ([Section 5, Pages + Frames = Page Tables](csbu-print_split_034.html#page_tables)
    which must be followed for virtual address translation to proceed. In response
    to access to a virtual-address that is not present in the TLB, the processor will
    automatically walk the page-tables to load the correct translation entry. Only
    if the translation entry does not exist will the processor raise an exception
    to be handled by the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the page-table traversal in specialised hardware gives speed advantages
    when finding translations, but removes flexibility from operating-systems implementors
    who might like to implement alternative schemes for page-tables.
  prefs: []
  type: TYPE_NORMAL
- en: All architectures can be broadly categorised into these two methodologies. Later,
    we will examine some common architectures and their virtual-memory support.
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 9 Linux Specifics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the basic concepts of virtual memory remain constant, the specifics
    of implementations are highly dependent on the operating system and hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Address Space Layout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linux divides the available address space up into a shared kernel component
    and private user space addresses. This means that addresses in the kernel port
    of the address space map to the same physical memory for each process, whilst
    user-space addresses are private to the process. On Linux, the shared kernel space
    is at the very top of the available address space. On the most common processor,
    the 32 bit x86, this split happens at the 3GB point. As 32 bits can map a maximum
    of 4GB, this leaves the top 1GB for the shared kernel regionThis is unfortunately
    an over-simplification, because many machines wanted to support more than 4GB
    per process. *High memory* support allows processors to get access to a full 4GB
    via special extensions..
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![The Linux address space layout. Note that pages in the user-space
    address space are private, whilst the kernel pages are shared.](linux-layout.svg)</picture>Figure 9.1.1 Linux
    address space layout
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Three Level Page Table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many different ways for an operating system to organise the page tables
    but Linux chooses to use a *hierarchical* system.
  prefs: []
  type: TYPE_NORMAL
- en: As the page tables use a hierarchy that is three levels deep, the Linux scheme
    is most commonly referred to as the *three level page table*. The three level
    page table has proven to be robust choice, although it is not without its criticism.
    The details of the virtual memory implementation of each processor vary Whitley
    meaning that the generic page table Linux chooses must be portable and relatively
    generic.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of the three level page table is not difficult. We already know
    that a virtual address consists of a page number and an offset in the physical
    memory page. In a three level page table, the virtual address is further split
    up into a number *levels*.
  prefs: []
  type: TYPE_NORMAL
- en: Each level is a page table of its own right; i.e. it maps a page number of a
    physical page. In a single level page table the "level 1" entry would directly
    map to the physical frame. In the multilevel version each of the upper levels
    gives the address of the physical memory frame holding the next lower levels page
    table.
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![A three level page table](threelevel.svg)</picture>Figure 9.2.1 Linux
    Three Level Page Table
  prefs: []
  type: TYPE_NORMAL
- en: So a sample reference involves going to the top level page table, finding the
    physical frame that the next level address is on, reading that levels table and
    finding the physical frame that the next levels page table lives on, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: At first, this model seems to be needlessly complex. The main reason this model
    is implemented is for size considerations. Imagine the theoretical situation of
    a process with only one single page mapped right near the end of its virtual address
    space. We said before that the page table entry is found as an offset from the
    page table base register, so the page table needs to be a contiguous array in
    memory. So the single page near the end of the address space requires the entire
    array, which might take up considerable space (many, many physical pages of memory).
  prefs: []
  type: TYPE_NORMAL
- en: In a three level system, the first level is only one physical frame of memory.
    This maps to a second level, which is again only a single frame of memory, and
    again with the third. Consequently, the three level system reduces the number
    of pages required to only a fraction of those required for the single level system.
  prefs: []
  type: TYPE_NORMAL
- en: There are obvious disadvantages to the system. Looking up a single address takes
    more references, which can be expensive. Linux understands that this system may
    not be appropriate on many different types of processor, so each architecture
    can *collapse* the page table to have less levels easily (for example, the most
    common architecture, the x86, only uses a two level system in its implementation).
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
- en: <main class="calibre3">
  prefs: []
  type: TYPE_NORMAL
- en: 10 Hardware support for virtual memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As covered in [Section 8.2, The TLB](csbu-print_split_037.html#the_tlb), the
    processor hardware provides a lookup-table that links virtual addresses to physical
    addresses. Each processor architecture defines different ways to manage the TLB
    with various advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: The part of the processor that deals with virtual memory is generally referred
    to as the *Memory Management Unit* or MMU
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 x86-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XXX
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Itanium
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Itanium MMU provides many interesting features for the operating system
    to work with virtual memory.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Address spaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Section 8.3.1, Flushing the TLB](csbu-print_split_037.html#flushing_tlb) introduced
    the concept of the *address-space ID* to reduce the overheads of flushing the
    TLB when context switching. However, programmers often use *threads* to allow
    execution contexts to share an address space. Each thread has the same ASID and
    hence shares TLB entries, leading to increased performance. However, a single
    ASID prevents the TLB from enforcing protection; sharing becomes an "all or nothing"
    approach. To share even a few bytes, threads must forgo all protection from each
    other (see also [Section 7.2, Protection](csbu-print_split_036.html#protection)).'
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![Itanium regions and protection keys. In this example the processes
    alias region 1\. Each process has a private mapping and they share a key for another.](ia64-regions-keys.svg)</picture>Figure 10.2.1.1 Illustration
    Itanium regions and protection keys
  prefs: []
  type: TYPE_NORMAL
- en: The Itanium MMU considers these problems and provides the ability to share an
    address space (and hence translation entries) at a much lower granularity whilst
    still maintaining protection within the hardware. The Itanium divides the 64-bit
    address space up into 8 *regions*, as illustrated in [Figure 10.2.1.1, Illustration
    Itanium regions and protection keys](#ia64_regions_keys). Each process has eight
    24-bit *region registers* as part of its state, which each hold a *region ID*
    (RID) for each of the eight regions of the process address space. TLB translations
    are tagged with the RID and thus will only match if the process also holds this
    RID, as illustrated in [Figure 10.2.1.2, Illustration of Itanium TLB translation](#ia64_tlb_translation).
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![Illustration of the Itanium translation process (Mosberger).](ia64-tlb-translation.svg)</picture>Figure 10.2.1.2 Illustration
    of Itanium TLB translation
  prefs: []
  type: TYPE_NORMAL
- en: Further to this, the top three bits (the region bits) are not considered in
    virtual address translation. Therefore, if two processes share a RID (i.e., hold
    the same value in one of their region registers) then they have an aliased view
    of that region. For example, if process-A holds RID `0x100` in region-register
    3 and process-B holds the same RID `0x100` in region-register 5 then process-A,
    region 3 is aliased to process-B, region 5. This limited sharing means both processes
    receive the benefits of shared TLB entries without having to grant access to their
    entire address space.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1.1 Protection Keys
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To allow for even finer grained sharing, each TLB entry on the Itanium is also
    tagged with a *protection key*. Each process has an additional number of *protection
    key registers* under operating-system control.
  prefs: []
  type: TYPE_NORMAL
- en: When a series of pages is to be shared (e.g., code for a shared system library),
    each page is tagged with a unique key and the OS grants any processes allowed
    to access the pages that key. When a page is referenced the TLB will check the
    key associated with the translation entry against the keys the process holds in
    its protection key registers, allowing the access if the key is present or otherwise
    raising a *protection* fault to the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: The key can also enforce permissions; for example, one process may have a key
    which grants write permissions and another may have a read-only key. This allows
    for sharing of translation entries in a much wider range of situations with granularity
    right down to a single-page level, leading to large potential improvements in
    TLB performance.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Itanium Hardware Page-Table Walker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Switching context to the OS when resolving a TLB miss adds significant overhead
    to the fault processing path. To combat this, Itanium allows the option of using
    built-in hardware to read the page-table and automatically load virtual-to-physical
    translations into the TLB. The hardware page-table walker (HPW) avoids the expensive
    transition to the OS, but requires translations to be in a fixed format suitable
    for the hardware to understand.
  prefs: []
  type: TYPE_NORMAL
- en: The Itanium HPW is referred to in Intel's documentation as the *virtually hashed
    page-table walker* or VHPT walker, for reasons which should become clear. Itanium
    gives developers the option of two mutually exclusive HPW implementations; one
    based on a virtual linear page-table and the other based on a hash table.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted it is possible to operate with no hardware page-table walker;
    in this case each TLB miss is resolved by the OS and the processor becomes a software-loaded
    architecture. However, the performance impact of disabling the HPW is so considerable
    it is very unlikely any benefit could be gained from doing so
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2.1 Virtual Linear Page-Table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The virtual linear page-table implementation is referred to in documentation
    as the *short format virtually hashed page-table* (SF-VHPT). It is the default
    HPW model used by Linux on Itanium.
  prefs: []
  type: TYPE_NORMAL
- en: The usual solution is a multi-level or hierarchical page-table, where the bits
    comprising the virtual page number are used as an index into intermediate levels
    of the page-table (see [Section 9.2, Three Level Page Table](csbu-print_split_038.html#three_level_page_table)).
    Empty regions of the virtual address space simply do not exist in the hierarchical
    page-table. Compared to a linear page-table, for the (realistic) case of a tightly-clustered
    and sparsely-filled address space, relatively little space is wasted in overheads.
    The major disadvantage is the multiple memory references required for lookup.
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![The hierarchical page-table](hierarchical-pt.svg)</picture>Figure 10.2.2.1.1 Illustration
    of a hierarchical page-table
  prefs: []
  type: TYPE_NORMAL
- en: With a 64-bit address space, even a 512~GiB linear table identified in [Section 6.3,
    Virtual Address Translation](csbu-print_split_035.html#virtual_address_translation)
    takes only 0.003% of the 16-exabytes available. Thus a *virtual linear page-table*
    (VLPT) can be created in a contiguous area of *virtual* address space.
  prefs: []
  type: TYPE_NORMAL
- en: Just as for a physically linear page-table, on a TLB miss the hardware uses
    the virtual page number to offset from the page-table base. If this entry is valid,
    the translation is read and inserted directly into the TLB. However, with a VLPT
    the address of the translation entry is itself a virtual address and thus there
    is the possibility that the virtual page which it resides in is not present in
    the TLB. In this case a *nested fault* is raised to the operating system. The
    software must then correct this fault by mapping the page holding the translation
    entry into the VLPT.
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![Operation of the Itanium short-format VHPT](ia64-short-format.svg)</picture>Figure 10.2.2.1.2 Itanium
    short-format VHPT implementation
  prefs: []
  type: TYPE_NORMAL
- en: This process can be made quite straight forward if the operating system keeps
    a hierarchical page-table. The leaf page of a hierarchical page-table holds translation
    entries for a virtually contiguous region of addresses and can thus be mapped
    by the TLB to create the VLPT as described in [Figure 10.2.2.1.2, Itanium short-format
    VHPT implementation](#ia64_short_format).
  prefs: []
  type: TYPE_NORMAL
- en: <picture>![Itanium PTE entry formats](ia64-ptes.svg)</picture>Figure 10.2.2.1.3 Itanium
    PTE entry formats
  prefs: []
  type: TYPE_NORMAL
- en: The major advantage of a VLPT occurs when an application makes repeated or contiguous
    accesses to memory. Consider that for a walk of virtually contiguous memory, the
    first fault will map a page full of translation entries into the virtual linear
    page-table. A subsequent access to the next virtual page will require the next
    translation entry to be loaded into the TLB, which is now available in the VLPT
    and thus loaded very quickly and without invoking the operating system. Overall,
    this will be an advantage if the cost of the initial nested fault is amortised
    over subsequent HPW hits.
  prefs: []
  type: TYPE_NORMAL
- en: The major drawback is that the VLPT now requires TLB entries which causes an
    increase on TLB pressure. Since each address space requires its own page table
    the overheads become greater as the system becomes more active. However, any increase
    in TLB capacity misses should be more than regained in lower refill costs from
    the efficient hardware walker. Note that a pathological case could skip over `page_size`
    ÷ `translation_size` entries, causing repeated nested faults, but this is a very
    unlikely access pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The hardware walker expects translation entries in a specific format as illustrated
    on the left of [Figure 10.2.2.1.3, Itanium PTE entry formats](#ia64_ptes). The
    VLPT requires translations in the so-called 8-byte *short format*. If the operating
    system is to use its page-table as backing for the VLPT (as in [Figure 10.2.2.1.2,
    Itanium short-format VHPT implementation](#ia64_short_format)) it must use this
    translation format. The architecture describes a limited number of bits in this
    format as ignored and thus available for use by software, but significant modification
    is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: A linear page-table is premised on the idea of a fixed page size. Multiple page-size
    support is problematic since it means the translation for a given virtual page
    is no longer at a constant offset. To combat this, each of the 8-regions of the
    address space ([Figure 10.2.1.1, Illustration Itanium regions and protection keys](#ia64_regions_keys))
    has a separate VLPT which only maps addresses for that region. A default page-size
    can be given for each region (indeed, with Linux HugeTLB, discussed below, one
    region is dedicated to larger pages). However, page sizes can not be mixed within
    a region.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2.2 Virtual Hash Table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using TLB entries in an effort to reduce TLB refill costs, as done with the
    SF-VHPT, may or may not be an effective trade-off. Itanium also implements a *hashed
    page-table* with the potential to lower TLB overheads. In this scheme, the processor
    *hashes* a virtual address to find an offset into a contiguous table.
  prefs: []
  type: TYPE_NORMAL
- en: The previously described physically linear page-table can be considered a hash
    page-table with a *perfect* hash function which will never produce a collision.
    However, as explained, this requires an impractical trade-off of huge areas of
    contiguous physical memory. However, constraining the memory requirements of the
    page table raises the possibility of collisions when two virtual addresses hash
    to the same offset. Colliding translations require a *chain* pointer to build
    a linked-list of alternative possible entries. To distinguish which entry in the
    linked-list is the correct one requires a *tag* derived from the incoming virtual
    address.
  prefs: []
  type: TYPE_NORMAL
- en: The extra information required for each translation entry gives rise to the
    moniker *long-format*~VHPT (LF-VHPT). Translation entries grow to 32-bytes as
    illustrated on the right hand side of [Figure 10.2.2.1.3, Itanium PTE entry formats](#ia64_ptes).
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this approach is the global hash table can be pinned with
    a single TLB entry. Since all processes share the table it should scale better
    than the SF-VHPT, where each process requires increasing numbers of TLB entries
    for VLPT pages. However, the larger entries are less cache friendly; consider
    we can fit four 8-byte short-format entries for every 32-byte long-format entry.
    The very large caches on the Itanium processor may help mitigate this impact,
    however.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of the SF-VHPT is that the operating system can keep translations
    in a hierarchical page-table and, as long as the hardware translation format is
    maintained, can map leaf pages directly to the VLPT. With the LF-VHPT the OS must
    either use the hash table as the primary source of translation entries or otherwise
    keep the hash table as a cache of its own translation information. Keeping the
    LF-VHPT hash table as a cache is somewhat sub-optimal because of increased overheads
    on time critical fault paths, however advantages are gained from the table requiring
    only a single TLB entry.
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
