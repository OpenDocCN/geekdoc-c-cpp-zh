<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better</h1>
<blockquote>原文：<a href="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/Chapter5.html">https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/Chapter5.html</a></blockquote>
      
      
      

      <h1 id="chapter-5-mysql-internals">Chapter 5: MySQL Internals</h1>

<p>To address the numerous problems inherent in MySQL, it is essential to have a solid foundation of knowledge related to MySQL. This chapter provides detailed explanations of MySQL core fundamentals.</p>

<h2 id="51-the-storage-stack-of-innodb">5.1 The “Storage Stack” of InnoDB</h2>

<p>The following figure depicts the InnoDB storage stack from a developer’s perspective. The upper layer primarily consists of the SQL layer, while the lower layer comprises the InnoDB storage engine layer with transaction capabilities. Interaction between the SQL layer and the InnoDB storage engine layer occurs through interfaces. The InnoDB storage engine primarily includes the transaction layer and the mini-transaction layer. InnoDB interacts with the operating system through system functions, and the operating system interacts with the hardware.</p>

<p><img src="../Images/91096b34b517bc15cea58a69f18b3a42.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/ab7e53a364737d98ee06c53aef3108b0.png"/></p>

<p>Figure 5-1. The InnoDB Storage Stack.</p>

<p>In the InnoDB storage engine, changes are applied through mini-transactions (mtr), which enable atomic modifications across multiple pages. This approach maintains data consistency during concurrent transactions and database anomalies. Since a single transaction often involves changes to multiple pages, mini-transactions ensure page-level consistency, meaning a single transaction typically comprises multiple mini-transactions.</p>

<p>The figure below illustrates the function call stack relationship between transactions and mini-transactions, showing how transactions use mini-transactions to execute low-level operations.</p>

<p><img src="../Images/5eabfef4a07c9d88d726c35feff2c693.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/f558d2899635882ec517e2b1da5a0a8f.png"/></p>

<p>Figure 5-2. Call stack relationship between transactions and mini-transactions.</p>

<p>It should be emphasized that the transaction layer and the mini-transaction layer together implement the functionality of a complete transaction.</p>

<h2 id="52-transactions">5.2 Transactions</h2>

<p>The ACID model outlines key database design principles essential for business data and mission-critical applications. MySQL, with components like the InnoDB storage engine, adheres closely to the ACID model to ensure data integrity and prevent corruption during exceptional conditions such as software crashes and hardware failures. Relying on ACID-compliant features eliminates the need for custom consistency checking and crash recovery mechanisms. However, in cases where additional safeguards exist, ultra-reliable hardware is used, or minor data loss or inconsistency is acceptable, MySQL settings can be adjusted to trade some ACID reliability for increased performance or throughput [13].</p>

<p>To implement a transaction, the following ACID properties must be satisfied:</p>

<ol>
  <li><strong>Atomicity</strong>: Ensures “all or nothing” semantics, meaning either all operations of a transaction are completed, or none are. This aspect mainly involves InnoDB transactions.</li>
  <li><strong>Consistency</strong>: Requires every transaction to maintain the predetermined integrity rules of the database, transforming it from one consistent state to another. Consistency is ensured by the DBMS and involves internal InnoDB processing to protect data from crashes.</li>
  <li><strong>Isolation</strong>: Prevents transactions from interfering with each other, ensuring incomplete transactions are not visible to others. Isolation is primarily managed through InnoDB transactions and the isolation level applied to each transaction.</li>
  <li><strong>Durability</strong>: Guarantees that once a transaction is committed, it remains so, even in the event of a crash. This aspect involves MySQL software features and the hardware configuration, and it is the most complex to provide specific guidelines for.</li>
</ol>

<p>In the InnoDB storage engine:</p>

<ul>
  <li><strong>Transaction Layer</strong>:
    <ul>
      <li><strong>Atomicity, Consistency, and Isolation</strong>: Achieved through locks and ReadView.</li>
      <li><strong>Cross-Engine Atomic Commits</strong>: Implemented using XA Two-Phase Commit (2PC), ensuring atomicity between SQL layer binlogs and InnoDB redo logs, forming the basis for crash recovery.</li>
    </ul>
  </li>
  <li><strong>Mini-Transaction Layer</strong>:
    <ul>
      <li><strong>Atomic, Consistent, and Durable Modifications</strong>: Managed through interactions with redo/undo logs across multiple pages, supporting crash recovery.</li>
    </ul>
  </li>
</ul>

<p>Overall, atomicity, consistency, and durability are jointly achieved through both the mini-transaction and transaction layers, while isolation is mainly managed at the transaction layer.</p>

<p>In InnoDB, each transaction is assigned a transaction ID that strictly increases in chronological order. Transaction IDs are generated not only by external transactions but also by various internal operations within MySQL, such as GTID updates triggering internal transactions for persistence.</p>

<h2 id="53-concurrency-control">5.3 Concurrency Control</h2>

<p>High-performance transactional systems require concurrent transactions to meet performance demands. Without concurrency control, these systems cannot provide correct results or maintain consistent databases [45].</p>

<p>Concurrency control allows end-users to access a database simultaneously while maintaining the illusion that each transaction runs independently on a dedicated system, ensuring atomicity and isolation.</p>

<p>Two-phase locking (2PL) was the first proven method for ensuring the correct execution of concurrent transactions in a database system. Under 2PL, transactions must acquire locks on database elements before reading or writing them. A transaction needs a read lock to read an element and a write lock to modify it.</p>

<p>Online Transaction Processing (OLTP) systems rely on concurrency control protocols to ensure the serializability of concurrently executed transactions. When two parallel transactions attempt to access the same data item, the concurrency control protocol coordinates their accesses to maintain serializability. Different protocols achieve this in various ways. Locking-based protocols, such as two-phase locking (2PL), associate a lock with each data item. A transaction must acquire all necessary locks (shared or exclusive) before releasing any. Validation-based protocols, such as optimistic concurrency control (OCC), execute a transaction with potentially stale or uncommitted data and validate for serializability before committing [51].</p>

<p>It is worth noting that concurrency control is only one of the several aspects of a DBMS that affects scalability [51].</p>

<h2 id="54-transaction-isolation-level">5.4 Transaction Isolation Level</h2>

<p>Transaction isolation is fundamental to database processing. Isolation, the “I” in ACID, balances performance, reliability, consistency, and reproducibility when multiple transactions occur simultaneously [45]. In InnoDB, the traditional four transaction isolation levels are implemented, focusing here on Repeatable Read (RR), Read Committed (RC), and Serializable levels.</p>

<p>MySQL’s default isolation level is Repeatable Read. In this level, a ReadView is obtained at the start of the transaction, ensuring consistent data reads throughout. The mechanism uses transaction ID information from ReadView to fetch the specified data version from the undo log, maintaining data consistency.</p>

<p>The most commonly used isolation level in MySQL is Read Committed, which is also Oracle’s default. All TPC-C tests in this book use the Read Committed level. In Read Committed, each read operation in a transaction acquires a corresponding ReadView, potentially resulting in different data for identical reads if concurrent modifications occur.</p>

<p>Serializable isolation provides the strongest form of isolation, similar to serial execution. However, Serializable isolation does not mandate serial execution; transactions can execute in parallel if they do not conflict.</p>

<h2 id="55-mvcc">5.5 MVCC</h2>

<p>Due to performance and other considerations, databases rarely implement isolation levels based solely on locks. The MVCC + lock method is the most popular implementation, as it allows read requests without locking.</p>

<p>Under MVCC (Multi-Version Concurrency Control), each write operation creates a new version of a tuple, tagged with the transaction’s timestamp. The DBMS maintains a list of versions for each element, determining which version a transaction will access during read operations. This ensures a serializable ordering of operations and prevents the rejection of read operations due to overwritten data.</p>

<p>InnoDB, a multi-version storage engine, keeps old versions of changed rows to support concurrency and rollback. This information is stored in undo tablespaces within rollback segments, which contain insert and update undo logs. Insert undo logs are needed only for transaction rollbacks and can be discarded upon commit. Update undo logs are used for consistent reads and can be discarded only when no transaction requires them for building an earlier version of a row.</p>

<p>Regularly committing transactions, including those with consistent reads, is recommended to prevent the rollback segment from growing excessively and filling up the undo tablespace in which it resides [13].</p>

<h2 id="56-innodb-architecture">5.6 InnoDB Architecture</h2>

<p>The following diagram illustrates the in-memory and on-disk structures that comprise the InnoDB storage engine architecture.</p>

<p><img src="../Images/93a8187af3ee6fb46ec37fcdc1f29026.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/af8ae3176c5521d1cf12466ec3e2b54d.png"/></p>

<p>Figure 5-3. InnoDB Architecture borrowed from the official MySQL documentation.</p>

<p><strong>In-Memory Structures</strong></p>

<ol>
  <li><strong>Buffer Pool</strong>:
    <ul>
      <li>Caches table and index data in main memory, allowing frequently accessed data to be read directly from memory, speeding up processing.</li>
      <li>Divided into pages to hold multiple rows, managed using a linked list and a variation of the least recently used (LRU) algorithm.</li>
      <li>Key aspect of MySQL tuning for efficient high-volume read operations.</li>
    </ul>
  </li>
  <li><strong>Log Buffer</strong>:
    <ul>
      <li>Holds data to be written to the log files on disk, periodically flushed to disk.</li>
      <li>A larger log buffer allows large transactions to run without writing redo log data to disk before committing, reducing disk I/O.</li>
      <li>Controlled by the <strong>innodb_flush_log_at_trx_commit</strong> variable.</li>
    </ul>
  </li>
</ol>

<p><strong>On-Disk Structures</strong></p>

<ol>
  <li><strong>Doublewrite Buffer</strong>:
    <ul>
      <li>An intermediate storage area where pages from the buffer pool are written before their final position in InnoDB data files.</li>
      <li>Ensures recovery from partial writes due to system crashes or unexpected shutdowns.</li>
      <li>Efficient as it doesn’t double the I/O overhead despite data being written twice.</li>
    </ul>
  </li>
  <li><strong>Redo Log</strong>:
    <ul>
      <li>Disk-based structure used for crash recovery, correcting data from incomplete transactions.</li>
      <li>Encodes changes from SQL statements or low-level API calls; replayed automatically during initialization after a crash.</li>
      <li>Optimizes random writes into sequential log writes (ARIES algorithm) [2], improving performance.</li>
      <li>Redo log files are crucial for acknowledging transaction completion.</li>
    </ul>
  </li>
  <li><strong>Undo Log</strong>:
    <ul>
      <li>Part of undo log segments within rollback segments, residing in undo tablespaces and the global temporary tablespace.</li>
      <li>Essential for transaction rollbacks and MVCC (Multi-Version Concurrency Control) reads.</li>
    </ul>
  </li>
</ol>

<p>By effectively managing these structures, InnoDB achieves a balance of high reliability and performance.</p>

<h2 id="57-log-manager">5.7 Log Manager</h2>

<p>The log manager is a critical component of modern DBMSs, often prone to bottlenecks due to its centralized design and dependence on I/O. Long flush times, log-induced latch contention, and contention for log buffers in main memory all impact scalability, with no single bottleneck solely responsible for suboptimal performance. Modern systems can achieve transaction rates of 100 ktps or higher, exacerbating the log bottleneck. Existing research offers partial solutions to these bottlenecks, but none provide a fully scalable log manager for today’s multicore hardware [3].</p>

<p>The log manager was a major scalability bottleneck in MySQL 5.7. However, MySQL 8.0 underwent significant restructuring in this area, leading to substantial improvements in scalability.</p>

<h2 id="58-lock-scheduling-algorithms">5.8 Lock Scheduling Algorithms</h2>

<p>In computing, scheduling is the action of assigning resources to perform tasks [45]. Scheduling algorithms are resource allocation strategies determined by the system’s needs, such as FIFO (First In, First Out), Round Robin, and Shortest Job First (SJF). These algorithms are used in operating systems, databases, and networks.</p>

<p>MySQL 5.7 utilized the classic FIFO lock scheduling algorithm. Later versions, starting with MySQL 8.0.20, adopted the CATS (Contention-Aware Transaction Scheduling) lock scheduling algorithm. The purpose of this change was to improve the overall efficiency of MySQL operations and improve throughput.</p>

<p>Let’s analyze the CATS algorithm used in MySQL 8.0. The core idea of the CATS algorithm is to prioritize locks for transactions with higher weighted costs when locks are released. The figure below illustrates the principle mechanism of CATS [57]. Despite transaction t1 having a deeper subgraph, CATS allocates the lock to t2 because completing t2 allows triggering more concurrent transactions to execute.</p>

<p><img src="../Images/b568d49d2705cde60e5532c0c9109a80.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/ce55033da131c4260c130ad3cb118bd5.png"/></p>

<p>Figure 5-4. Lock scheduling example borrowed from the Paper ‘Contention-Aware Lock Scheduling for Transactional Databases’.</p>

<p>The CATS algorithm theoretically has significant effectiveness in scenarios with severe lock contention. It has shown some impact in SysBench Pareto distribution test scenarios, but the exact extent of its effectiveness depends on specific circumstances.</p>

<p>Chapter 7 will subsequently provide a detailed discussion of the CATS scheduling algorithm.</p>

<h2 id="59-binlog-file">5.9 Binlog File</h2>

<p>MySQL improves its versatility by introducing binlog files at the SQL layer to record transaction modifications, facilitating data replication and disaster recovery. For MySQL transactions, changes are first written to the binlog files and then to the redo log files, with atomicity ensured through the XA Two-Phase Commit (2PC) mechanism.</p>

<p>Binlog files are crucial for data replication and high availability, supporting asynchronous replication, semisynchronous replication, and Group Replication.</p>

<p>This book focuses on row-based binlog, where transactions are stored as events.</p>

<p>With the advent of fast solid-state drives and techniques like group commit, the impact of log flush I/O times has lessened.</p>

<h2 id="510-group-commit-mechanism">5.10 Group Commit Mechanism</h2>

<p>MySQL introduced the binlog group commit mechanism to reduce the number of disk I/O operations by combining multiple binlog flush operations when multiple transactions are committed simultaneously. This approach reduces disk I/O by postponing log access to stable storage, gathering multiple commits in memory, and issuing a single write and flush for a set of transactions.</p>

<p>Group commit strategies improve disk performance by aggregating multiple log flush requests into a single I/O operation, reducing the frequency of disk accesses. However, group commit does not eliminate unwanted context switches, as transactions block pending notification from the log rather than blocking directly on I/O requests [3]. Efficient activation mechanisms are needed to reduce context switches, but the current MySQL implementation is inefficient. This problem will be thoroughly explored in Chapter 8.</p>

<h2 id="511-execution-plan">5.11 Execution Plan</h2>

<p>An execution plan details how a SQL statement is executed after optimization by the MySQL query optimizer. Depending on the table structure, indexes, and WHERE clause conditions, the optimizer considers various techniques to perform efficient lookups. Queries on large tables can be executed without reading all rows, and joins can be performed without comparing every row combination.</p>

<p>The MySQL query optimizer is designed for simple, OLTP-type queries and has limitations with complex queries. For instance, join order optimization in practical applications uses only left-deep plans and a greedy algorithm. The figure below illustrates MySQL query optimization and execution architecture [44]. The Parser and Solver layers handle syntax checking, name resolution, access control, data types, and string collations. During the Prepare phase, logical transformations occur, such as merging derived tables, predicate pushdown, and converting subqueries.</p>

<p><img src="../Images/7bf617906da900d574bf7cfc1cbeb12a.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/72624869e303c62430942b0c55c4897a.png"/></p>

<p>Figure 5-5. MySQL query optimization and execution architecture borrowed from the Paper ‘Integrating the Orca Optimizer into MySQL’.</p>

<p>Cost-based Optimization, which is limited to one SELECT block at a time, determines the best join order, join method, and table access method. The optimizer generally considers only left-deep plans and performs aggregation after all tables are joined. Plan Refinement involves pushing selection conditions into tables and indexes, avoiding sorts if index scans deliver sorted rows, and adding aggregations, group-level filtering, and row limit enforcement.</p>

<p>Heuristics might miss the optimal plan, leading to higher execution times. Join Order Optimization has been extensively studied, with parallel approaches developed for multicore architectures. Due to the NP-hard nature of join order optimization, heuristic solutions and limited search spaces are used.</p>

<p>In general, there is still a lot of optimization potential in MySQL’s execution plans. MySQL continues to explore new approaches, which have been reflected in MySQL 8.0.</p>

<h2 id="512-partitioning">5.12 Partitioning</h2>

<p>Partitioning allows you to distribute table data across a file system based on rules you define, effectively storing different parts of a table as separate tables in various locations. This division, governed by a partitioning function, can use modulus, range or list matching, internal hashing, or linear hashing. The function, specified by the user, takes a user-supplied expression as its parameter, which can be a column value, a function acting on one or more column values, or a set of column values [13].</p>

<p>The benefits of using partitioning are as follows:</p>

<ol>
  <li>Enabling storage of more data than a single disk or file system partition can hold.</li>
  <li>Simplifying data management by allowing easy removal of obsolete data through dropping partitions, and facilitating the addition of new data by adding partitions.</li>
  <li>Optimizing queries by limiting searches to specific partitions that contain relevant data.</li>
</ol>

<p>MySQL partitioning not only offers these benefits but also reduces latch contention for large tables under high concurrency. The following figure shows the impact on TPC-C throughput after partitioning a large table in BenchmarkSQL.</p>

<p><img src="../Images/f7ee06d391e2889b41c0edf6557909f2.png" alt="image-20240829091316193" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829091316193.png"/></p>

<p>Figure 5-6. Comparison of BenchmarkSQL tests before and after partitioning.</p>

<p>The figure shows that partitioning has minimal impact under low concurrency. However, when concurrency exceeds 150, partitioning significantly improves throughput by alleviating latch conflicts in large tables.</p>

<p>Unless stated otherwise, all TPC-C tests in this book use partitioned large tables. Each table has its own latch, and partitioning employs latch sharding to reduce latch conflicts under high concurrency, preventing latch contention from affecting performance tests on large tables.</p>

<h2 id="513-coordination-avoidance">5.13 Coordination Avoidance</h2>

<p>Minimizing coordination between concurrently executing operations is crucial for maximizing scalability, availability, and performance in database systems. However, coordination-free execution can compromise application correctness and consistency. While serializable transactions maintain correctness, they are not necessary for all applications and can limit scalability [25].</p>

<h2 id="514-disaster-recovery">5.14 Disaster Recovery</h2>

<p>Disaster recovery ensures a database can be brought back online after an outage. For MySQL, this involves timely flushing of binlog and redo logs, as well as writing to the doublewrite buffer to prevent recovery problems caused by damaged data pages.</p>

<h2 id="515-idempotence">5.15 Idempotence</h2>

<p>Database code that creates or alters tables and routines should be idempotent to avoid problems if applied multiple times. Idempotence prevents duplicate data creation during sync failures by recording progress by batch, rather than by individual record. When a sync is interrupted, the process must often restart at the beginning of the last batch, leading to reprocessing some data.</p>

<p>Here’s an example of MySQL secondary replay. In the following code snippet, MySQL achieves idempotence during the replay process on the secondary.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">const</span> <span class="kt">bool</span> <span class="n">skip_transaction</span> <span class="o">=</span> <span class="n">is_already_logged_transaction</span><span class="p">(</span><span class="n">thd</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">gtid_next_list</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">skip_transaction</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">skip_statement</span><span class="p">(</span><span class="n">thd</span><span class="p">);</span>
      <span class="k">return</span> <span class="n">GTID_STATEMENT_SKIP</span><span class="p">;</span>
    <span class="p">}</span>     
    <span class="k">return</span> <span class="n">GTID_STATEMENT_EXECUTE</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</code></pre></div></div>

<p>The <em>is_already_logged_transaction</em> function is called to determine if a transaction has already been executed. If it has, <em>skip_transaction</em> is set to true. Consequently, the subsequent process immediately returns <em>GTID_STATEMENT_SKIP</em>, halting further replay of the transaction.</p>

<h2 id="516-thread-pool">5.16 Thread Pool</h2>

<p>MySQL executes statements using one thread per client connection. As the number of connections increases beyond a certain threshold, performance degrades. The following figure shows the TPC-C throughput versus concurrency testing for MySQL 5.7.36.</p>

<p><img src="../Images/a2d5501d728e0c7808a80ec3b454910f.png" alt="image-20240829091450756" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829091450756.png"/></p>

<p>Figure 5-7. TPC-C throughput vs. concurrency in MySQL 5.7.36.</p>

<p>After a concurrency level of 75, the throughput sharply declines, confirming the above conclusion.</p>

<p>Thread pooling reuses a fixed number of threads to handle multiple client connections, reducing overhead and avoiding contention and context switching [31]. The MySQL thread pool separates user connections from threads. Each user connection no longer has a dedicated OS thread. Instead, the thread pool consists of thread groups, with a default of <em>n</em> groups. User connections are assigned to a thread group in a round-robin fashion. Each thread group manages a subset of connections, with one or more threads executing queries from those connections.</p>

<p><img src="../Images/42168ccda380f9de8185193fd8f98a16.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/a86c70fc9172e988971f8a4a6fdc5bd0.png"/></p>

<p>Figure 5-8. Thread pool model borrowed from the MySQL blog.</p>

<p>The Percona thread pool was widely used in MySQL 5.7, but with MySQL 8.0’s improved scalability, its role has diminished. MySQL 8.0 introduced new thread pool modes designed to prevent performance degradation as user connections increase. The “Max Transaction Limit” feature limits the number of concurrently executing transactions, which improves overall throughput by reducing data locks and deadlocks on heavily loaded systems [31]. Thus, controlling user thread entry into the InnoDB storage engine is key to alleviating MySQL scalability problems.</p>

<h2 id="517-traditional-cluster">5.17 Traditional Cluster</h2>

<p>Traditional MySQL cluster relies on asynchronous and semisynchronous replication, which are straightforward and easy for maintenance personnel to manage.</p>

<h3 id="5171-asynchronous-replication">5.17.1 Asynchronous Replication</h3>

<p>Traditional MySQL replication uses a simple source to replica approach, with the primary applying transactions and then asynchronously sending them to the secondaries to be applied. This shared-nothing system ensures all servers have a full copy of the data by default [13].</p>

<p>Asynchronous replication offers better write scalability but at the cost of lower data coherence. The following figure is the flowchart of asynchronous replication [13]. The primary continues executing without waiting for acknowledgment from the secondary, resulting in user SQL query response times comparable to a single server. However, this can lead to data loss if the primary fails before the secondary has received the latest data.</p>

<p><img src="../Images/5463731060c48d28007534db3ccc580a.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/6ac0aaccb67a8a3975345dd133a30e0d.png"/></p>

<p>Figure 5-9. MySQL asynchronous replication borrowed from the official MySQL documentation.</p>

<p>When asynchronous replication is used, if the primary fails and a new leader is chosen, unreplicated writes from the old leader may be lost. This can cause conflicts and durability problems. Often, the solution is to discard the old leader’s unreplicated writes, which may not meet clients’ durability expectations. If both leaders accept writes without conflict resolution, there’s a risk of both nodes shutting down if not properly managed [28].</p>

<h3 id="5172-semisynchronous-replication">5.17.2 Semisynchronous Replication</h3>

<p>To address data loss in asynchronous replication, MySQL introduced semisynchronous replication. With semisynchronous replication, a transaction commit requires the corresponding binlog to be delivered to at least one MySQL secondary before proceeding. This ensures that at least one secondary has the most recent data.</p>

<p>In MySQL semisynchronous replication, the secondary sends an ACK reply to the primary only after the relay log is written to disk. The primary waits for at least one ACK reply before continuing the transaction. This introduces extra latency from network time, as well as the secondary processing binlog events and writing them to disk.</p>

<p>In traditional high availability setups, semisynchronous replication can be cumbersome and complex. Meta, for instance, has highlighted these problems in their implementation of high availability based on the Raft protocol [38].</p>

<p>Since semisynchronous replication alone doesn’t fully address high availability problems, many third-party tools have emerged, and MySQL has introduced Group Replication.</p>

<h3 id="5173-how-scalable-is-semisynchronous-replication">5.17.3 How Scalable is Semisynchronous Replication?</h3>

<p>Here is the relevant code showing the process semisynchronous replication goes through before sending an ACK response:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">io_slave_killed</span><span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">mi</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">ulong</span> <span class="n">event_len</span><span class="p">;</span>
        <span class="p">...</span>
        <span class="n">THD_STAGE_INFO</span><span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">stage_waiting_for_source_to_send_event</span><span class="p">);</span>
        <span class="n">event_len</span> <span class="o">=</span> <span class="n">read_event</span><span class="p">(</span><span class="n">mysql</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rpl</span><span class="p">,</span> <span class="n">mi</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">suppress_warnings</span><span class="p">);</span>
        <span class="p">...</span>
        <span class="n">THD_STAGE_INFO</span><span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">stage_queueing_source_event_to_the_relay_log</span><span class="p">);</span>
        <span class="n">event_buf</span> <span class="o">=</span> <span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">mysql</span><span class="o">-&gt;</span><span class="n">net</span><span class="p">.</span><span class="n">read_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
        <span class="p">...</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">RUN_HOOK</span><span class="p">(</span><span class="n">binlog_relay_io</span><span class="p">,</span> <span class="n">after_read_event</span><span class="p">,</span>
                     <span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">mi</span><span class="p">,</span> <span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">mysql</span><span class="o">-&gt;</span><span class="n">net</span><span class="p">.</span><span class="n">read_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">event_len</span><span class="p">,</span>
                      <span class="o">&amp;</span><span class="n">event_buf</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">event_len</span><span class="p">)))</span> <span class="p">{</span>
          <span class="n">mi</span><span class="o">-&gt;</span><span class="n">report</span><span class="p">(</span><span class="n">ERROR_LEVEL</span><span class="p">,</span> <span class="n">ER_REPLICA_FATAL_ERROR</span><span class="p">,</span>
                     <span class="n">ER_THD</span><span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">ER_REPLICA_FATAL_ERROR</span><span class="p">),</span>
                     <span class="s">"Failed to run 'after_read_event' hook"</span><span class="p">);</span>
          <span class="k">goto</span> <span class="n">err</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="p">...</span>
        <span class="n">QUEUE_EVENT_RESULT</span> <span class="n">queue_res</span> <span class="o">=</span> <span class="n">queue_event</span><span class="p">(</span><span class="n">mi</span><span class="p">,</span> <span class="n">event_buf</span><span class="p">,</span> <span class="n">event_len</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">queue_res</span> <span class="o">==</span> <span class="n">QUEUE_EVENT_ERROR_QUEUING</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">mi</span><span class="o">-&gt;</span><span class="n">report</span><span class="p">(</span><span class="n">ERROR_LEVEL</span><span class="p">,</span> <span class="n">ER_REPLICA_RELAY_LOG_WRITE_FAILURE</span><span class="p">,</span>
                     <span class="n">ER_THD</span><span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">ER_REPLICA_RELAY_LOG_WRITE_FAILURE</span><span class="p">),</span>
                     <span class="s">"could not queue event from source"</span><span class="p">);</span>
          <span class="k">goto</span> <span class="n">err</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="p">...</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">RUN_HOOK</span><span class="p">(</span><span class="n">binlog_relay_io</span><span class="p">,</span> <span class="n">after_queue_event</span><span class="p">,</span>
                     <span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">mi</span><span class="p">,</span> <span class="n">event_buf</span><span class="p">,</span> <span class="n">event_len</span><span class="p">,</span> <span class="n">synced</span><span class="p">)))</span> <span class="p">{</span>
          <span class="n">mi</span><span class="o">-&gt;</span><span class="n">report</span><span class="p">(</span><span class="n">ERROR_LEVEL</span><span class="p">,</span> <span class="n">ER_REPLICA_FATAL_ERROR</span><span class="p">,</span>
                     <span class="n">ER_THD</span><span class="p">(</span><span class="n">thd</span><span class="p">,</span> <span class="n">ER_REPLICA_FATAL_ERROR</span><span class="p">),</span>
                     <span class="s">"Failed to run 'after_queue_event' hook"</span><span class="p">);</span>
          <span class="k">goto</span> <span class="n">err</span><span class="p">;</span>  
        <span class="p">}</span> 
        <span class="p">...</span>
        <span class="n">thd</span><span class="o">-&gt;</span><span class="n">mem_root</span><span class="o">-&gt;</span><span class="n">ClearForReuse</span><span class="p">();</span>
      <span class="p">}</span> 
</code></pre></div></div>

<p>In the binlog file, a transaction consists of multiple events. For a TPC-C transaction, it is normal to have dozens of events. Each event goes through processes like <em>read event</em>, <em>after_read_event</em>, <em>queue event</em>, and <em>after_queue_event</em>. The more events a transaction contains, the longer the processing time, and all these events are processed by a single thread. This limitation in single-threaded processing, coupled with event-based handling, means that semisynchronous replication has limited computational capacity and poor scalability.</p>

<p>The following figure shows the TPC-C throughput versus concurrency testing for semisynchronous replication. It can be observed that the scalability of semisynchronous replication is very weak, far inferior to that of refactored Group Replication.</p>

<p><img src="../Images/73326d43429f2d13ece98746845a6a15.png" alt="image-20240829091539440" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829091539440.png"/></p>

<p>Figure 5-10. Performance comparison between Group Replication with Paxos log persistence and semisynchronous replication.</p>

<h2 id="518-group-replication">5.18 Group Replication</h2>

<p>For continuous operation, a business requires high availability of its databases. To ensure continuous availability, a database must be fault-tolerant and robust to withstand failures. These qualities are achieved by Group Replication [13].</p>

<h3 id="5181-why-implement-group-replication">5.18.1 Why Implement Group Replication?</h3>

<p>Asynchronous and semisynchronous replication cannot fully address high availability complexities. To achieve high availability, MySQL uses state machine replication based on the Paxos algorithm, known as Group Replication. This method theoretically solves the high availability problems that other replication methods cannot. Despite its potential, Group Replication faces numerous challenges, which is why it hasn’t gained widespread popularity.</p>

<h3 id="5182-why-was-mencius-initially-adopted">5.18.2 Why Was Mencius Initially Adopted?</h3>

<p>Mencius is a multi-leader state machine replication protocol derived from Paxos [32]. It is designed to achieve high throughput under high client load and low latency under low client load, adapting to changing network and client environments. Mencius partitions the sequence of consensus protocol instances among servers, amortizing the leader load and increasing throughput when CPU-bound. It also fully utilizes available bandwidth and reduces latency by allowing clients to use a local server as the leader. Due to these advantages, Mencius aligns with the multi-primary mode design of Group Replication, where each MySQL node can perform write operations at any time, and was initially adopted by MySQL.</p>

<h3 id="5183-why-introduce-the-single-leader-multi-paxos-algorithm">5.18.3 Why Introduce the Single-Leader Multi-Paxos Algorithm?</h3>

<p>The single leader Multi-Paxos algorithm has the following characteristics [13]:</p>

<ul>
  <li>It relies on a single leader to choose the request sequence.</li>
  <li>This simplicity results in high throughput and low latency for clients near the leader but higher latency for clients further away.</li>
  <li>The leader becomes a bottleneck, limiting throughput and creating an unbalanced communication pattern that underutilizes available network bandwidth.</li>
</ul>

<p>MySQL introduced the single leader Multi-Paxos algorithm to improve performance and resilience in single-primary mode, especially when some secondary members are unreachable [13].</p>

<p>Tests in cross-datacenter scenarios show that using the single leader Multi-Paxos algorithm significantly improves Group Replication performance in single-primary mode. However, it has the drawback of high latency for other nodes needing to send data, as they must obtain the request sequence from the leader.</p>

<h3 id="5184-does-group-replication-lose-data">5.18.4 Does Group Replication Lose Data?</h3>

<p>Group Replication implements state machine replication but does not inherently include durable state machine replication, meaning Paxos messages are not persisted. This design choice implies that while consensus can be reached within the cluster, data loss is possible in extreme cases. For instance, if all Group Replication nodes crash simultaneously and the MySQL primary cannot be restarted, the cluster formed by the remaining nodes may lose transaction data that had not been written to disk.</p>

<h3 id="5185-will-group-replication-outperform-semisynchronous-replication">5.18.5 Will Group Replication Outperform Semisynchronous Replication?</h3>

<p>The Mencius algorithm theoretically enables Group Replication to reach in-memory consensus, bypassing the need to parse transaction events at the Paxos layer. Additionally, batching mechanisms can merge several transactions into a single message for Paxos communication. Based on this, the throughput of Group Replication is expected to surpass that of semisynchronous replication.</p>

<h3 id="5186-is-a-single-thread-sufficient-for-underlying-paxos-communication">5.18.6 Is a Single Thread Sufficient for Underlying Paxos Communication?</h3>

<p>Paxos operates serially, but with pipelining and batching, it significantly improves throughput. Therefore, even with multi-threaded Paxos communication, Group Replication’s overall system throughput is generally unaffected. However, large transactions can overwhelm a single thread, making multiple threads necessary to reduce wait times and speed up processing. In summary, while a single thread is sufficient for typical transactions, handling a large volume of large transactions may require multiple threads.</p>

<h3 id="5187-paxos-single-leader-vs-group-replication-single-primary-mode">5.18.7 Paxos Single Leader vs. Group Replication Single-Primary Mode</h3>

<p>Group Replication uses two Paxos variants: single leader Multi-Paxos and multiple leader Mencius. In single-primary mode, only one node handles transactional updates, while in multi-primary mode, all nodes can perform updates. The figure below illustrates the relationship between these algorithms and Group Replication’s application modes:</p>

<p><img src="../Images/9bf645fd4a4e0bf58fc123a01672101b.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/612e4a7da5eea52195e8994a9b22413c.png"/></p>

<p>Figure 5-11. Relationship between Paxos variant algorithms and Group Replication modes.</p>

<p>In single-primary mode, both the Mencius algorithm and the single-leader Multi-Paxos algorithm can be used. However, in multi-primary mode, only the Mencius algorithm is applicable. This is because using the single-leader Multi-Paxos algorithm in multi-primary mode would severely degrade performance, as every non-leader node would need to request sequences from the leader node.</p>

<p>Group Replication in single-primary mode adopts the single-leader Multi-Paxos algorithm, assuming the leader node primarily sends messages while non-leader nodes have minimal roles. This allows efficient use of the single-leader Multi-Paxos algorithm, ensuring effective coordination and consensus among nodes.</p>

<h3 id="5188-is-single-leader-multi-paxos-universally-applicable-in-single-primary-mode">5.18.8 Is Single Leader Multi-Paxos Universally Applicable in Single-Primary Mode?</h3>

<p>In Group Replication’s single-primary mode, consistent read operations require each MySQL node to send ‘before’ messages, while consistent write operations require ‘after’ messages. Using the single-leader Multi-Paxos algorithm in this context would significantly degrade performance.</p>

<p>The following figure illustrates SysBench’s throughput for 100 concurrent read/write operations over time, with MySQL configured for strong consistency writes using the “after” mechanism.</p>

<p><img src="../Images/02d2a52564001366c91b2e2912ec7623.png" alt="image-20240829091801038" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829091801038.png"/></p>

<p>Figure 5-12. Performance of strong consistency writes using the “after” mechanism under single leader Paxos variant.</p>

<p>From the figure, it can be observed that after adopting the single leader Multi-Paxos, the throughput of MySQL’s primary with strong consistency writes is significantly low. The following figure shows a partial screenshot of SysBench test results, indicating an average response time of 73ms.</p>

<p><img src="../Images/8738c0653ab343e2f6ad61e7c8e1dc8e.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/27e726f3ea50b5cabf751ee31a417b82.png"/></p>

<p>Figure 5-13. Partial screenshot of SysBench test results using the “after” mechanism under single leader Paxos variant.</p>

<p>Users reading official documentation might assume that the single leader Multi-Paxos algorithm can accelerate access. However, in reality, this algorithm is not suitable for consistent read/write operations.</p>

<h2 id="519-mysql-secondary-replay">5.19 MySQL Secondary Replay</h2>

<h3 id="5191-introduction-to-mysql-secondary-replay">5.19.1 Introduction to MySQL Secondary Replay</h3>

<p>MySQL supports primary/secondary replication, typically using log shipping. In this setup, a primary instance generates logs for transactions, which are then shipped to one or more secondary instances. The secondaries replay these logs to mirror the primary’s state, but may lag behind, affecting read-only queries’ accuracy [16].</p>

<p>As server failures become common, replication for high availability is crucial. However, the need for high concurrency in transaction processing on multicore systems conflicts with traditional replication methods. This tension can lead to secondaries falling behind under heavy primary loads, increasing the risk of data loss or requiring throttling of the primary, thus impacting performance.</p>

<h3 id="5192-the-difference-between-replay-and-transaction-execution">5.19.2 The Difference Between Replay and Transaction Execution</h3>

<p>In MySQL’s row-based logging, each operation is a log event. Row changes are recorded as insert, update, or delete events. These row events, along with transaction start and end events, define transaction boundaries. Insert events log new row images, update events log both before and after row images, and delete events log deleted row images.</p>

<p>During transactions, the primary writes updates to the log, which the secondary then fetches and replays. Secondaries can handle more read requests than the primary because replaying updates incurs only about half the workload of executing the original query. Additionally, read queries on the primary can conflict with update transactions, causing slowdowns, which supports dispatching read requests to secondaries [16].</p>

<h3 id="5193-the-role-of-speeding-up-mysql-secondary-replay">5.19.3 The Role of Speeding Up MySQL Secondary Replay</h3>

<p>The faster the MySQL secondary replay speed, the lower the likelihood of reading stale data from the MySQL secondary. For Group Replication, the speed of MySQL secondary replay is closely tied to high availability. If the MySQL secondary replay speed is fast enough, during a high availability switch, the new primary can immediately start serving new requests. Otherwise, it typically needs to wait until MySQL secondary replay is completed before it can serve new requests.</p>

<h3 id="5194-the-architecture-of-mysql-secondary-replay">5.19.4 The Architecture of MySQL Secondary Replay</h3>

<p>The architecture of MySQL secondary replay is shown in the figure below:</p>

<p><img src="../Images/e9f82de3372c656a2fa5eb5d124f5d0b.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/56b6bb93cdc7ee8a05a1c6924f8d95c6.png"/></p>

<p>Figure 5-14. MySQL secondary replay architecture.</p>

<p>Asynchronous and semisynchronous replication both utilize an IO thread to read and store transaction events into the relay log. When Group Replication operates normally, it uses the applier thread to store applier events into the relay log. The SQL thread is crucial for MySQL secondary replay, responsible for parsing events and scheduling them. During MySQL secondary replay, multiple workers handle the replay process, each with its own worker queue where the SQL thread places pending events.</p>

<p>For MySQL secondary replay, the SQL thread acts not only as the scheduler but also reads and parses transaction events from the relay log files. When the relay log volume is small, the SQL thread can manage, but as the relay log grows, the SQL thread becomes the primary bottleneck. It struggles to keep up with the workload of parsing events and managing scheduling tasks. Moreover, the SQL thread encounters waiting situations under the following conditions:</p>

<ol>
  <li>Each worker queue has a fixed size with no adjustable parameters. If a transaction contains numerous events (e.g., large transactions), the worker queue quickly fills up, causing the SQL thread to wait.</li>
  <li>If there aren’t enough workers available, the SQL thread waits.</li>
  <li>If the SQL thread finds a new transaction with a last committed value greater than the minimum logical timestamp (low-water-mark) of committed transactions (LWM value), it also needs to wait.</li>
</ol>

<p>For example, the following code snippet illustrates how the SQL thread enters a waiting state when the worker queue is full.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">// possible WQ overfill</span>
  <span class="k">while</span> <span class="p">(</span><span class="n">worker</span><span class="o">-&gt;</span><span class="n">running_status</span> <span class="o">==</span> <span class="n">Slave_worker</span><span class="o">::</span><span class="n">RUNNING</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">thd</span><span class="o">-&gt;</span><span class="n">killed</span> <span class="o">&amp;&amp;</span>
         <span class="p">(</span><span class="n">ret</span> <span class="o">=</span> <span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs</span><span class="p">.</span><span class="n">en_queue</span><span class="p">(</span><span class="n">job_item</span><span class="p">))</span> <span class="o">==</span>
             <span class="n">Slave_jobs_queue</span><span class="o">::</span><span class="n">error_result</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">thd</span><span class="o">-&gt;</span><span class="n">ENTER_COND</span><span class="p">(</span><span class="o">&amp;</span><span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs_cond</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs_lock</span><span class="p">,</span>
                    <span class="o">&amp;</span><span class="n">stage_replica_waiting_worker_queue</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">old_stage</span><span class="p">);</span>
    <span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs</span><span class="p">.</span><span class="n">overfill</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
    <span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs</span><span class="p">.</span><span class="n">waited_overfill</span><span class="o">++</span><span class="p">;</span>
    <span class="n">rli</span><span class="o">-&gt;</span><span class="n">mts_wq_overfill_cnt</span><span class="o">++</span><span class="p">;</span>
    <span class="c1">// wait if worker queue is full</span>
    <span class="n">mysql_cond_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs_cond</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs_lock</span><span class="p">);</span>
    <span class="n">mysql_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs_lock</span><span class="p">);</span>
    <span class="n">thd</span><span class="o">-&gt;</span><span class="n">EXIT_COND</span><span class="p">(</span><span class="o">&amp;</span><span class="n">old_stage</span><span class="p">);</span>
    <span class="n">mysql_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">worker</span><span class="o">-&gt;</span><span class="n">jobs_lock</span><span class="p">);</span>
  <span class="p">}</span>
</code></pre></div></div>

<p>Here is another example of code where the SQL thread needs to wait if it detects a newly started transaction with a last committed value greater than the currently calculated LWM value:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">bool</span> <span class="n">Mts_submode_logical_clock</span><span class="o">::</span><span class="n">wait_for_last_committed_trx</span><span class="p">(</span>
    <span class="n">Relay_log_info</span> <span class="o">*</span><span class="n">rli</span><span class="p">,</span> <span class="n">longlong</span> <span class="n">last_committed_arg</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">THD</span> <span class="o">*</span><span class="n">thd</span> <span class="o">=</span> <span class="n">rli</span><span class="o">-&gt;</span><span class="n">info_thd</span><span class="p">;</span>
  <span class="p">...</span>
  <span class="k">if</span> <span class="p">((</span><span class="o">!</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">info_thd</span><span class="o">-&gt;</span><span class="n">killed</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">is_error</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
      <span class="o">!</span><span class="n">clock_leq</span><span class="p">(</span><span class="n">last_committed_arg</span><span class="p">,</span> <span class="n">get_lwm_timestamp</span><span class="p">(</span><span class="n">rli</span><span class="p">,</span> <span class="nb">true</span><span class="p">)))</span> <span class="p">{</span>
    <span class="n">PSI_stage_info</span> <span class="n">old_stage</span><span class="p">;</span>
    <span class="k">struct</span> <span class="nc">timespec</span> <span class="n">ts</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
    <span class="n">set_timespec_nsec</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">assert</span><span class="p">(</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">gaq</span><span class="o">-&gt;</span><span class="n">get_length</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">);</span>  <span class="c1">// there's someone to wait</span>
    <span class="n">thd</span><span class="o">-&gt;</span><span class="n">ENTER_COND</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">logical_clock_cond</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">mts_gaq_LOCK</span><span class="p">,</span>
                    <span class="o">&amp;</span><span class="n">stage_worker_waiting_for_commit_parent</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">old_stage</span><span class="p">);</span>
    <span class="k">do</span> <span class="p">{</span>
      <span class="c1">// wait if LWM is less than last committed</span>
      <span class="n">mysql_cond_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">logical_clock_cond</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">mts_gaq_LOCK</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">while</span> <span class="p">((</span><span class="o">!</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">info_thd</span><span class="o">-&gt;</span><span class="n">killed</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">is_error</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
             <span class="o">!</span><span class="n">clock_leq</span><span class="p">(</span><span class="n">last_committed_arg</span><span class="p">,</span> <span class="n">estimate_lwm_timestamp</span><span class="p">()));</span>
    <span class="n">min_waited_timestamp</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">SEQ_UNINIT</span><span class="p">);</span>  <span class="c1">// reset waiting flag</span>
    <span class="n">mysql_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">mts_gaq_LOCK</span><span class="p">);</span>
    <span class="n">thd</span><span class="o">-&gt;</span><span class="n">EXIT_COND</span><span class="p">(</span><span class="o">&amp;</span><span class="n">old_stage</span><span class="p">);</span>
    <span class="n">set_timespec_nsec</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ts</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">rli</span><span class="o">-&gt;</span><span class="n">mts_total_wait_overlap</span> <span class="o">+=</span> <span class="n">diff_timespec</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ts</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">min_waited_timestamp</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">SEQ_UNINIT</span><span class="p">);</span>
    <span class="n">mysql_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rli</span><span class="o">-&gt;</span><span class="n">mts_gaq_LOCK</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">rli</span><span class="o">-&gt;</span><span class="n">info_thd</span><span class="o">-&gt;</span><span class="n">killed</span> <span class="o">||</span> <span class="n">is_error</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Therefore, the SQL thread is often busy and encounters many waiting situations, which is actually one of the main reasons for slow MySQL secondary replay.</p>

<h2 id="520-the-integration-of-ai-with-databases">5.20 The Integration of AI with Databases</h2>

<p>Traditional database design relies on empirical methods and specifications, requiring human involvement (e.g., DBAs) for tuning and maintenance. AI techniques alleviate these limitations by exploring more design space than humans and replacing heuristics to solve complex problems. The existing AI techniques for optimizing databases can be categorized as follows [55].</p>

<h3 id="5201-learning-based-database-configuration">5.20.1 Learning-based Database Configuration</h3>

<ol>
  <li>
    <p><strong>Knob Tuning</strong></p>

    <p>Databases have numerous knobs that need to be tuned by DBAs for different scenarios. This approach is not scalable for millions of cloud database instances. Recently, learning-based techniques have been used to automatically tune these knobs, exploring more combinations and recommending high-quality settings, often outperforming DBAs.</p>
  </li>
  <li>
    <p><strong>Index/View Advisor</strong></p>

    <p>Indexes and views are essential for high performance, traditionally managed by DBAs. Given the vast number of column/table combinations, recommending and building appropriate indexes/views is costly. Recently, learning-based approaches have emerged to automate the recommendation and maintenance of indexes and views.</p>
  </li>
  <li>
    <p><strong>SQL Rewriter</strong></p>

    <p>Many SQL programmers struggle to write high-quality queries, necessitating rewrites for performance improvement. For example, nested queries may be rewritten as joins for optimization. Existing methods use rule-based strategies, relying on predefined rules, which are limited by the quality and scalability of the rules. Deep reinforcement learning can be used to select and apply rules effectively.</p>
  </li>
</ol>

<h3 id="5202-learning-based-database-optimization">5.20.2 Learning-based Database Optimization</h3>

<ol>
  <li>
    <p><strong>Cardinality/Cost Estimation</strong></p>

    <p>Traditional database optimizers struggle to capture correlations between different columns/tables, leading to suboptimal cost and cardinality estimations. Recently, deep learning techniques have been proposed to improve these estimations by using neural networks to better capture correlations.</p>
  </li>
  <li>
    <p><strong>Join Order Selection</strong></p>

    <p>SQL queries can have millions or even billions of possible execution plans. Efficiently finding a good plan is crucial, but traditional optimizers struggle with large tables due to the high cost of exploring vast plan spaces. Deep reinforcement learning methods have been developed to automatically select efficient plans.</p>
  </li>
  <li>
    <p><strong>End-to-End Optimizer</strong></p>

    <p>A comprehensive optimizer must consider cost/cardinality estimation, join order, indexes, and views. Learning-based optimizers use deep neural networks to optimize SQL queries holistically, improving overall query performance.</p>
  </li>
</ol>

<h3 id="5203-learning-based-database-design">5.20.3 Learning-based Database Design</h3>

<p>Traditional databases are designed by architects based on experience, which limits the exploration of design spaces. Recently, learning-based self-design techniques have emerged [55]:</p>

<ol>
  <li><strong>Learned indexes</strong>: These reduce index size and improve performance.</li>
  <li><strong>Learned data structure design</strong>: Different data structures suit different environments (e.g., hardware, read/write applications). Data structure alchemy creates an inference engine to recommend and design suitable structures.</li>
  <li><strong>Learning-based Transaction Management</strong>: Traditional techniques focus on protocols like OCC, PCC, MVCC, 2PC. New studies use AI to predict and schedule transactions, balancing conflict rates and concurrency by learning from data patterns and predicting future workload trends.</li>
</ol>

<h3 id="5204-learning-based-database-monitoring">5.20.4 Learning-based Database Monitoring</h3>

<p>Database monitoring captures runtime metrics such as read/write latency and CPU/memory usage, alerting administrators to anomalies like performance slowdowns and attacks. Traditional methods rely on administrators to monitor activities and report problems, which is inefficient. Machine learning techniques optimize this process by determining when and how to monitor specific metrics.</p>

<h3 id="5205-learning-based-database-security">5.20.5 Learning-based Database Security</h3>

<p>Traditional database security techniques, such as data masking and auditing, rely on user-defined rules and cannot automatically detect unknown vulnerabilities. AI-based algorithms address this by:</p>

<ol>
  <li><strong>Sensitive Data Discovery</strong>: Automatically identifying sensitive data using machine learning.</li>
  <li><strong>Anomaly Detection</strong>: Monitoring database activities to detect vulnerabilities.</li>
  <li><strong>Access Control</strong>: Automatically estimating data access actions to prevent data leaks.</li>
  <li><strong>SQL Injection Prevention</strong>: Using deep learning to analyze user behavior and identify SQL injection attacks.</li>
</ol>

<h3 id="5206-performance-prediction">5.20.6 Performance Prediction</h3>

<p>Query performance prediction is crucial for meeting service level agreements (SLAs), especially for concurrent queries. Traditional methods focus only on logical I/O metrics, neglecting many resource-related features, leading to inaccurate results.</p>

<p>Marcus et al. used deep learning to predict query latency under concurrency, accounting for interactions between child/parent operators and parallel plans. However, their pipeline structure caused information loss and failed to capture operator-to-operator relations like data sharing and conflict features.</p>

<p>To address this, Zhou et al. proposed a method using graph embedding. They modeled concurrent queries with a graph where vertices represent operators and edges capture operator correlations (e.g., data passing, access conflicts, resource competition). A graph convolution network was used to embed the workload graph, extract performance-related features, and predict performance based on these features [55].</p>

<h3 id="5207-ai-challenges">5.20.7 AI Challenges</h3>

<p>AI models require large-scale, high-quality, diversified training data for optimal performance, but obtaining such data in AI4DB is challenging due to security concerns and reliance on DBAs. For instance, in database knob tuning, training samples depend on DBA experience, making it difficult to gather sufficient samples. Effective models also need data covering various scenarios, hardware environments, and workloads, necessitating methods that perform well with small training datasets.</p>

<p>Adaptability is a major challenge, including adapting to dynamic data updates, different datasets, new hardware environments, and other database systems [55]. Key questions include:</p>

<ul>
  <li>How to adapt a trained model (e.g., optimizer, cost estimation) to other datasets?</li>
  <li>How to adapt a model to different hardware environments?</li>
  <li>How to adapt a model across different databases?</li>
  <li>How to support dynamic data updates?</li>
</ul>

<p>Model convergence is crucial. If a model doesn’t converge, alternative solutions are needed to avoid delays and inaccuracies, such as in knob tuning where non-converged models can’t provide reliable online suggestions.</p>

<p>Traditional OLAP focuses on relational data analytics, but big data introduces new types like graph, time-series, and spatial data. New techniques are required to analyze these multi-model data types and integrate AI with DB techniques for enhanced analytics, such as image analysis.</p>

<p>Transaction modeling and scheduling are critical for OLTP systems due to potential conflicts between transactions. Learning techniques can optimize OLTP queries, like consistent snapshots. However, efficient models are needed to instantly model and schedule transactions across multiple cores and machines.</p>

<h3 id="5208-ai-summary">5.20.8 AI Summary</h3>

<p>Integrating AI into MySQL offers many impactful opportunities and is one of the main focuses for future development.</p>

<h2 id="521-how-mysql-internals-work-in-a-pipeline-fashion">5.21 How MySQL Internals Work in a Pipeline Fashion?</h2>

<p>Database engines like MySQL excel at medium concurrency, interleaving the execution of many transactions, most of which are idle at any given moment [3]. However, as the number of cores per chip increases with Moore’s law, MySQL must exploit high parallelism to benefit from new hardware. Despite high concurrency in workloads, internal bottlenecks often prevent MySQL from achieving the needed parallelism.</p>

<p>MySQL’s internal operations follow a pipelining approach, where each component functions methodically. To ensure correctness, MySQL uses latches and locks to prevent interference between concurrent operations. For crash-safe recovery, MySQL employs mechanisms like redo log, undo log, idempotence, and double write.</p>

<p>To support concurrent read and write operations, MySQL implements transaction isolation levels and MVCC. These mechanisms enable efficient handling of concurrent operations.</p>

<p>To mitigate single-server failure risks, MySQL uses clustering and high availability solutions. Some operations, such as redo log flushing and MVCC ReadView replication, are serialized. Fast execution of these processes supports high throughput, but slow serialization can hinder scalability.</p>

<p>MySQL manages various processes, including SQL parsing, execution plan generation, redo log writing, transaction subsystem operations, lock subsystem operations, binlog group commit, and network interactions. Effective throughput is achieved when these processes are handled by different threads without overlap. However, thread aggregation in a serialized process can lead to conflicts and performance problems.</p>

<p>Optimizing MySQL’s efficiency involves faster response times, reduced CPU consumption, and enhanced scalability.</p>

<h2 id="522-why-mysql-needs-to-support-high-concurrency">5.22 Why MySQL Needs to Support High Concurrency?</h2>

<p>The inability to achieve high throughput is primarily due to the constraints of guaranteeing ACID. More cores lead to more concurrent transactions accessing the same contended data, requiring serialized access to ensure isolation. The maximum throughput of a database system depends on factors like hardware and software implementation. While better hardware and software can improve performance, throughput is fundamentally limited by contended operations. Contended operations vary with transaction isolation levels, multiversioning capabilities, and operation commutativity. Regardless, some operations will always require serialization, limiting throughput. Adding more processors only increases throughput if additional transactions do not conflict with existing ones [23].</p>

<p>In high-conflict scenarios, linear scalability of throughput is unachievable. However, scalability improves in low conflict scenarios, as evidenced by TPC-C tests with numerous warehouses. In practical MySQL operations, short bursts of high concurrency are common. Supporting high concurrency helps maintain MySQL stability during runtime.</p>

<p>Subsequently, the significance of supporting high concurrency specifically for TPC-C testing is analyzed. The figure below illustrates the relationship between throughput and concurrency for MySQL 5.7.39 under a 1ms thinking time scenario. In this context, 1ms thinking time means the user waits 1ms before sending the next request after receiving the response from the previous one.</p>

<p><img src="../Images/c9d89222e3df75d3deee5f9946d3cd5c.png" alt="image-20240829092027870" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092027870.png"/></p>

<p>Figure 5-15. MySQL 5.7.39 poor scalability with 1ms thinking time.</p>

<p>From the figure, it can be observed that under a 1ms thinking time scenario, the throughput of MySQL 5.7.39 increases linearly at low concurrency levels. However, once it reaches 250 concurrency, the throughput sharply declines.</p>

<p>The following figure shows the relationship between throughput and concurrency for improved MySQL 8.0.27 under the same 1ms thinking time scenario:</p>

<p><img src="../Images/a473ac53eea5c13752d75acd2d5cb17f.png" alt="image-20240829092050429" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092050429.png"/></p>

<p>Figure 5-16. Excellent scalability of improved MySQL 8.0.27 with 1ms thinking time.</p>

<p>From the figure, it can be seen that the peak throughput is reached at 1000 concurrency, and this peak significantly exceeds the peak of MySQL 5.7.39 version.</p>

<p>High concurrency support in MySQL is not only crucial for scenarios with thinking time but also essential for deployments across multiple cities (geographically distributed deployments).</p>

<p>The following figure illustrates the relationship between throughput and concurrency for improved MySQL 8.0.27 under different network latency scenarios.</p>

<p><img src="../Images/75166a109fb95f1d7b30306d383e286a.png" alt="image-20240829092158912" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092158912.png"/></p>

<p>Figure 5-17. Excellent scalability of improved MySQL 8.0.27 under different network latencies.</p>

<p>In scenarios with network latency in the tens of microseconds range for localhost access, peak throughput is achieved at 250 concurrency. With a network latency of 5ms, peak throughput is reached at 500 concurrency, and with a 10ms latency, it requires 1000 concurrency.</p>

<p>Therefore, enhancing MySQL’s scalability is highly meaningful. In low-conflict scenarios, enhancing scalability significantly improves throughput and has profound implications across various application contexts. However, in high-conflict situations, transaction throttling strategies are necessary to mitigate scalability problems.</p>

<h2 id="523-understanding-mysql-cluster-scalability">5.23 Understanding MySQL Cluster Scalability</h2>

<p>First, let’s examine the scalability of asynchronous replication. The following figure illustrates the relationship between throughput and concurrency for both a standalone MySQL instance and asynchronous replication.</p>

<p><img src="../Images/3289831e08c974081c7823ff94dfcfe8.png" alt="image-20240829092329794" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092329794.png"/></p>

<p>Figure 5-18. Excellent scalability of MySQL asynchronous replication.</p>

<p>From the figure, it can be seen that with asynchronous replication, the MySQL secondary does not significantly impact the throughput of the MySQL primary.</p>

<p>Let’s now examine the scalability of semisynchronous replication. For example, the following figure illustrates the relationship between throughput and concurrency for semisynchronous replication.</p>

<p><img src="../Images/21bfcc4378bea2e5241fdbdbfc835d1a.png" alt="image-20240829092612681" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092612681.png"/></p>

<p>Figure 5-19. Poor scalability of MySQL semisynchronous replication.</p>

<p>From the figure, it is clear that the throughput of semisynchronous replication struggles to increase further. After reaching 100 concurrency, the throughput gradually declines, suggesting a barrier that limits additional scaling. For more details, please refer to section 5.17.3.</p>

<p>Finally, let’s examine the scalability of Group Replication. Before delving into that, it’s important to understand the performance drawbacks of state machine replication [41], as explained below:</p>

<p><em>However, the communication and synchronization cost of agreement entails that state-machine replication adds a considerable overhead to the system’s performance, i.e., typical state-machine replication request rates are lower than the request rates of non-replicated systems.</em></p>

<p>Subsequently, the maximum scalability of Group Replication is tested, including the implementation of transaction throttling mechanisms to ensure that only a specified number of threads can access the transaction system.</p>

<p>The following figure shows the relationship between Group Replication throughput and concurrency:</p>

<p><img src="../Images/77aac650c87a11b03ad5b089ab0e58a9.png" alt="image-20240829092656320" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092656320.png"/></p>

<p>Figure 5-20. Excellent scalability of improved Group Replication with transaction throttling mechanisms.</p>

<p>When throughput peaks at 250 concurrency and then slowly declines, it clearly demonstrates that Group Replication exhibits excellent scalability under the transaction throttling mechanism. Compared to semisynchronous replication, Group Replication’s design is significantly superior.</p>

<p>It is important to note that transaction throttling is implemented to manage the broad range of concurrency. An excessive number of transactions entering the InnoDB storage engine can degrade performance and interfere with the evaluation of Group Replication’s scalability.</p>

<h2 id="524-mysql-problem-diagnosis">5.24 MySQL Problem Diagnosis</h2>

<p>Bugs are an inevitable part of programming. Logical reasoning is crucial for identifying and fixing these bugs. It involves systematically analyzing the code, diagnosing the causes of problems, and devising a plan to address them based on logical deductions and inferences. This approach helps programmers debug and troubleshoot in a structured and effective manner [56].</p>

<p>In addition to the commonly used GDB debugging, another approach is to use logging to assist in troubleshooting problems. There will be case studies introducing this method in the subsequent chapters.</p>

<p>Besides the methods mentioned above, MySQL itself also provides several debugging solutions, which are described as follows:</p>

<p>1）MySQL is compiled with debug mode enabled, and trace output can be added in the command terminal (refer to the example below).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set global debug= 'd:T:t:i:o,/tmp/mysql.trace';
</code></pre></div></div>

<p>Executing specific SQL statements will output MySQL’s internal function call relationships to the trace file, as shown in the   example below:</p>

<p><img src="../Images/61be4f1b34bbb1d35f8172fc03541d1a.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/7979628d61c6921f54b584d93f88a0bf.png"/></p>

<p>Trace information is very helpful for understanding how MySQL operates, but this method is only suitable for scenarios with very low traffic volume.</p>

<p>2）The Performance Schema collects statistics on various types of information, including memory usage information, locks, and statistics on condition variables, among others.</p>

<p><img src="../Images/9a42f61b4b93e9a9038bed67505e85fc.png" alt="" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/fcbcd21ab48a370aa550ce594ade1794.png"/></p>

<p>The figure above uses Performance Schema to perform statistical analysis on mutexes and other elements, making it easy to identify which ones incur higher costs.</p>

<h2 id="525-the-significant-differences-between-benchmarksql-and-sysbench">5.25 The Significant Differences Between BenchmarkSQL and SysBench</h2>

<p>Using the case of optimizing lock-sys as an example, this section evaluates the significant differences between the SysBench tool and BenchmarkSQL in MySQL performance testing.</p>

<p>First, use SysBench’s standard read/write tests to evaluate the optimization of lock-sys.</p>

<p><img src="../Images/b29090872284cbdbfd1faed25b7a9c21.png" alt="image-20240829092940314" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092940314.png"/></p>

<p>Figure 5-21. Comparison of SysBench read-write tests before and after lock-sys optimization.</p>

<p>From the figure, it can be observed that after optimization, the overall performance of the SysBench tests has actually decreased.</p>

<p>Next, using BenchmarkSQL to test this optimization, the results are shown in the following figure.</p>

<p><img src="../Images/ff3dbfa2caedb8fe8f1755edfda03f08.png" alt="image-20240829092959775" style="zoom:150%;" data-original-src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/image-20240829092959775.png"/></p>

<p>Figure 5-22. Comparison of BenchmarkSQL tests before and after lock-sys optimization.</p>

<p>From the figure, it can be seen that the results of BenchmarkSQL’s TPC-C test indicate that the lock-sys optimization is effective. Why does such a significant difference occur? Let’s analyze the differences in characteristics between these testing tools to understand why their tests differ.</p>

<p>SysBench RW testing is characterized by its speed and simplicity with SQL queries. Under the same concurrency conditions, SysBench typically handles fewer concurrent transactions compared to BenchmarkSQL. Therefore, in the face of latch queue bottlenecks like lock-sys, high concurrency in SysBench may equate to low concurrency in BenchmarkSQL. Consequently, lock-sys optimizations may not have a significant impact in scenarios where BenchmarkSQL operates at lower concurrency levels.</p>

<p>BenchmarkSQL, a widely used TPC-C testing tool, distributes user threads more evenly across various modules, reducing susceptibility to aggregation effects. In high-concurrency situations, optimizing lock-sys can significantly reduce latch conflicts and minimize impact on other queues, thereby improving throughput. BenchmarkSQL’s TPC-C testing is better suited for uncovering deeper concurrency problems in MySQL compared to SysBench.</p>

<p>This analysis uses deductive reasoning to explore the differences between SysBench and BenchmarkSQL. It demonstrates that poor performance in SysBench tests does not necessarily indicate poor performance in production environments, and vice versa. This discrepancy arises because SysBench test environments often differ significantly from real-world production environments. Consequently, SysBench test results should be used for scenario-specific performance comparisons rather than as comprehensive indicators of production capabilities.</p>

<p>It is worth noting that the main basis for performance testing and comparison in this book, mainly based on TPC-C, is as follows [50]:</p>

<p><em>TPC benchmark C also known as TPC-C which is the leading online transaction processing (OLTP) benchmark has been used to perform the comparison.</em></p>

<p>In this book, BenchmarkSQL is predominantly used for TPC-C testing. This choice is based not only on BenchmarkSQL’s representative TPC-C testing capabilities but also on its higher alignment with real-world online environments.</p>

<p><a href="/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/Chapter6.html">Next</a></p>


      
        
</body>
</html>