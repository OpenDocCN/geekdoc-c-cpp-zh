- en: The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/Chapter8.html](https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/Chapter8.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Chapter 8: Refining MySQL 8.0: Next-Level Improvements'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter examines optimization opportunities and solutions found in MySQL
    8.0 improvements, including SQL layer improvements, InnoDB storage engine enhancements,
    transaction throttling mechanisms, and improvements in read and write performance
    for low concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Improvements at the SQL Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance degradation problem in certain scenarios related to query execution
    plans was fixed. The mechanism activating user threads in binlog group commit
    was also improved, further improving efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Solved Performance Degradation in Query Execution Plans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During secondary development on MySQL 8.0.27, TPC-C tests with BenchmarkSQL
    became unstable. Throughput rapidly declined, complicating optimization. Trust
    in the official version led to initially overlooking this problem despite testing
    difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: Only after a user reported a significant performance drop following an upgrade
    did we begin to take it seriously. The reliable feedback from users indicated
    that while MySQL 8.0.25 performed well, upgrading to MySQL 8.0.29 led to a substantial
    decline. This crucial information indicated that there was a performance problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simultaneously, it was confirmed that the performance degradation problem in
    MySQL 8.0.27 was the same as in MySQL 8.0.29\. MySQL 8.0.27 had undergone two
    scalability optimizations specifically for trx-sys, which theoretically should
    have increased throughput. Reviewing the impact of latch sharding in trx-sys on
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829102323261](../Images/4d71032cd37aa0881e0ffd416fa3f9de.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Impact of latch sharding in trx-sys under different concurrency
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue examining the comparison of throughput and concurrency between
    trx-sys latch sharding optimization and the MySQL 8.0.27 release version. Specific
    details are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829102344815](../Images/e95bbab82b6fa4a743016fc885f2c7aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Performance degradation in MySQL 8.0.27 release version.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is evident that the performance degradation of the MySQL
    8.0.27 release version is significant under low concurrency conditions, with a
    noticeable drop in peak performance. This aligns with user feedback regarding
    decreased throughput and is easily reproducible using BenchmarkSQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MySQL 8.0.27 release version already had this problem, whereas the earlier
    MySQL 8.0.25 release version did not. Using this information, the goal was to
    identify the specific git commit that caused the performance degradation. Finding
    the git commit responsible for performance degradation is a complex process that
    typically involves binary search. After extensive testing, it was initially narrowed
    down to a specific commit. However, this commit contained tens of thousands of
    lines of code, making it nearly impossible to pinpoint the exact segment causing
    the problem. It was later discovered that this commit was a collective merge from
    a particular branch. This allowed for further breakdown and ultimately identifying
    the root cause of the problem in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the commit information, two versions were compiled and SQL queries performing
    exceptionally slow in TPC-C tests were identified. The execution plans of these
    slow SQL queries were analyzed using *‘explain’*. Specific details are shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5a58f9e4c7f0722c97d2b126f5aa230.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Abnormalities indicated by rows in *‘explain’*.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it can be seen that most of the execution plans are identical,
    except for the *‘rows’* column. In the normal version, the *‘rows’* column shows
    just over 200, whereas in the problematic version, it shows over 1,000,000\. After
    continuously simplifying the SQL, a highly representative SQL query was finally
    identified. Specific details are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9517156aa8e41eb66e105f7fe29e112.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Significant discrepancies between SQL execution results and *‘explain’*
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the *Filter* information obtained from ‘*explain*’, the last query
    shown in the figure was constructed. The figure reveals that while the last query
    returned only 193 rows, ‘*explain*’ displayed over 1.17 million rows for *‘rows’*.
    This discrepancy highlights a complex problem, as execution plans are not always
    fully understood by all MySQL developers. Fortunately, identifying the commit
    responsible for the performance degradation provided a critical foundation for
    solving the problem. Although solving the problem was relatively straightforward
    with this information, analyzing the root cause from the SQL statement itself
    proved to be far more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue with an in-depth analysis of this problem. The following figure
    displays the ‘*explain*’ result for a specific SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0c1b3f120ff1b645df8d3ecf211f60a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Sample SQL query representing the problem.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it can be seen that the number of rows is still large, indicating
    that this SQL query is representative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two different debug versions of MySQL were compiled: one with anomalies and
    one normal. Debug versions were used to capture useful function call relationships
    through debug traces. When executing the problematic SQL statement on the version
    with anomalies, the relevant debug trace information is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d457871adfb51bb8dc45ef27b112ab96.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Debug trace information for the abnormal version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the normal version, the relevant debug trace information is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bfe987a002ee9a829e58c5629c647ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Debug trace information for the normal version.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two figures above, it is noticeable that the normal version includes
    additional content within the green box, indicating that conditions are applied
    in the normal version, whereas the abnormal version lacks these conditions. To
    understand why the abnormal version is missing these conditions, it is necessary
    to add additional trace information in the *get_full_func_mm_tree* function to
    capture specific details about the cause of this difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding extra trace information, the debug trace result for the abnormal
    version is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f23b53de0a2c47c64c3d55b9a296c85d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Supplementary debug trace information for the abnormal version.
  prefs: []
  type: TYPE_NORMAL
- en: 'The debug trace result for the normal version is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a02fc8312995d4e81a1a718fc0adaed.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Supplementary debug trace information for the normal version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon comparing the two figures above, significant differences are observed.
    In the normal version, the value of *param_comp* is 16140901064495857660, while
    in the abnormal version, it is 16140901064495857661, differing by 1\. To understand
    this discrepancy, let’s first examine how the *param_comp* value is calculated,
    as detailed in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From the code, it’s evident that *param_comp* is calculated using a bitwise
    OR operation on three variables, followed by a bitwise NOT operation. The difference
    of 1 suggests that at least one of these variables differs, helping to narrow
    down the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation involves three *table_map* variables with lengthy values, making
    ordinary calculators insufficient and the process too complex to detail here.
  prefs: []
  type: TYPE_NORMAL
- en: The key point is that debug tracing revealed critical differences. Coupled with
    the information provided by identifying the Git commit responsible for the performance
    discrepancy, analyzing the root cause is no longer difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the final fix patch, detailed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/347286e58d50ba1d185d866d32aa24d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Final patch for solving performance degradation in query execution
    plans.
  prefs: []
  type: TYPE_NORMAL
- en: When calling the *test_quick_select* function, reintroduce the *const_table*
    and *read_tables* variables (related to the previously discussed variables). This
    ensures that filtering conditions in the execution plan are not overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying the above patch to MySQL 8.0.27, the performance degradation
    problem was solved. A test comparing TPC-C throughput at various concurrency levels,
    both before and after applying the patch, was conducted. Specific details are
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829102642856](../Images/4bbab050fca78f2a95f3426db6660cd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Effects of the patch on solving performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is evident that after applying the patch, throughput and
    peak performance have significantly improved under low concurrency conditions.
    However, under high concurrency conditions, throughput not only failed to increase
    but actually decreased, likely due to scalability bottlenecks in MVCC ReadView.
  prefs: []
  type: TYPE_NORMAL
- en: 'After addressing the MVCC ReadView scalability problem, reassess the impact
    of this patch, as detailed in the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829102703396](../Images/d70207a29b6daa2efb165543cfd1fcd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Actual effects of the patch after addressing the MVCC ReadView
    scalability problem.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is evident that this patch has significantly improved MySQL’s
    throughput. This case demonstrates that scalability problems can disrupt certain
    optimizations. To scientifically assess the effectiveness of an optimization,
    it is essential to address most scalability problems beforehand to achieve a more
    accurate evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s examine the results of the long-term stability testing for TPC-C.
    The following figure shows the results of an 8-hour test under 100 concurrency,
    with throughput captured at various hours (where 1 ≤ n ≤ 8).
  prefs: []
  type: TYPE_NORMAL
- en: '![image-degrade4](../Images/75ac3f88fa5adeef6835bc836b6eb8be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-13\. Comparison of stability tests: MySQL 8.0.27 vs. improved MySQL
    8.0.27.'
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is evident that after applying the patch, the rate of throughput
    decline has been significantly mitigated. The MySQL 8.0.27 version experienced
    a dramatic throughput decline, failing to meet the stability requirements of TPC-C
    testing. However, after applying the patch, MySQL’s performance returned to normal.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing this problem directly presents considerable challenges, particularly
    for MySQL developers unfamiliar with query execution plans. Using logical reasoning
    and a systematic approach to identify and address code differences before and
    after the problem arose is a more elegant problem-solving method, though it is
    complex.
  prefs: []
  type: TYPE_NORMAL
- en: It is noteworthy that no regression testing problems were encountered after
    applying the patch, demonstrating high stability and providing a solid foundation
    for future performance improvements. Currently, MySQL 8.0.40 still hasn’t solved
    this problem, suggesting potential shortcomings in MySQL’s testing system. Given
    the complexity of MySQL databases, users should exercise caution when upgrading
    and consider using tools like TCPCopy [65] to avoid potential regression testing
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Improving Binlog Group Commit Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The binlog group commit mechanism is quite complex, and this complexity makes
    it challenging to identify its inherent performance problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, capture performance problems during the TPC-C test with 500 concurrency
    using the *perf* tool, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab128d40d411ca9a7e2059f7d33fe130.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. *_pthread_mutex_cond_lock* bottleneck reveals performance problems.
  prefs: []
  type: TYPE_NORMAL
- en: It is evident that *_pthread_mutex_cond_lock* is a significant bottleneck, accounting
    for approximately 9.5% of the overhead. Although *perf* does not directly pinpoint
    the exact problem, it indicates the presence of this bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: To address the problem, an in-depth exploration of MySQL internals was conducted
    to uncover the factors contributing to this performance bottleneck. A conventional
    binary search approach with minimal logging was used to identify functions or
    code segments that incur significant overhead during execution. The minimal logging
    approach was chosen to reduce performance interference while diagnosing the root
    cause of the problem. Excessive logging can disrupt performance analysis, and
    while some may use MySQL’s internal mechanisms for troubleshooting, these often
    introduce substantial performance overhead themselves.
  prefs: []
  type: TYPE_NORMAL
- en: After thorough investigation, the bottleneck was identified within the following
    code segment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Numerous occurrences of ‘wait too long’ output indicate that the bottleneck
    has been exposed. To investigate why ‘wait too long’ is being reported, the logs
    were added and modified accordingly. See the specific code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After another round of testing, a peculiar phenomenon was observed: when ‘wait
    too long’ messages appeared, the ‘wake up thread’ logs showed that many user threads
    were awakened multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem was traced to the *thd->tx_commit_pending* value not changing,
    causing threads to repeatedly re-enter the wait process. Further inspection reveals
    the conditions under which this variable becomes false, as illustrated in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: From the code, it is evident that *thd->tx_commit_pending* is set to false in
    the *signal_done* function. The *mysql_cond_broadcast* function then activates
    all waiting threads, leading to a situation similar to a thundering herd problem.
    When all previously waiting user threads are activated, they check if tx_commit_pending
    has been set to false. If it has, they proceed with processing; otherwise, they
    continue waiting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the complexity of the binlog group commit mechanism, a straightforward
    analysis identifies the root cause: threads that should not be activated are being
    triggered, leading to unnecessary context switches with each activation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During one test, additional statistics were collected on the number of times
    user threads entered the wait state. The details are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829103131857](../Images/7c5a76690408559fc34b6c40d72bb8b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-15\. Statistics of threads activated 1, 2, 3 times.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting once is normal and indicates 100% efficiency. Waiting twice suggests
    50% efficiency, and waiting three times indicates 33.3% efficiency. Based on the
    figure, the overall activation efficiency is calculated to be 52.7%.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, an ideal solution would be a multicast activation mechanism
    with 100% efficiency, where user threads with tx_commit_pending set to false are
    activated together. However, implementing this requires a deep understanding of
    the complex logic behind binlog group commit.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, a point-to-point activation mechanism is used, achieving 100%
    efficiency but introducing significant system call overhead. The following figure
    illustrates the relationship between TPC-C throughput and concurrency before and
    after optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829103236734](../Images/d48990eaef9215af54c8cc0e1fed399a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-16\. Impact of group commit optimization with innodb_thread_concurrency=128.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is evident that with innodb_thread_concurrency=128, the
    optimization of binlog group commit significantly improves throughput under high
    concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that this optimization’s effectiveness can vary depending
    on factors such as configuration settings and specific scenarios. However, overall,
    it notably improves throughput, especially in high concurrency conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is the comparison of TPC-C throughput and concurrency before and after
    optimization using standard configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829103259992](../Images/011494781b05f76ddf3a5377482bd79f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-17\. Impact of group commit optimization using standard configurations.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is clear that this optimization is less pronounced compared
    to the previous one, but it still shows overall improvement. Extensive testing
    indicates that the worse the scalability of MySQL, the more significant the effectiveness
    of binlog group commit optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, the previously identified bottleneck of *_pthread_mutex_cond_lock*
    has been significantly alleviated after optimization, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2b0c08856a0f3b019c50ffc1df2720a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-18\. Mitigation of *_pthread_mutex_cond_lock* bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this optimization helps address scalability problems associated
    with binlog group commit.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Enhancing the InnoDB Storage Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '8.2.1 MVCC ReadView: Identified Problems'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A key component of any MVCC scheme is the mechanism for quickly determining
    which tuples are visible to which transactions. A transaction’s snapshot is created
    by building a ReadView (RV) vector that holds the TXIDs of all concurrent transactions
    smaller than the transaction’s TXID. The cost of acquiring a snapshot increases
    linearly with the number of concurrent transactions, even if the transaction only
    reads tuples written by a single committed transaction, highlighting a known scalability
    limitation [7].
  prefs: []
  type: TYPE_NORMAL
- en: 'After understanding the scalability problems with the MVCC ReadView mechanism,
    let’s examine how MySQL implements MVCC ReadView. Under the Read Committed isolation
    level, during the process of reading data, the InnoDB storage engine triggers
    the acquisition of the ReadView. A screenshot of part of the ReadView data structure
    is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, *m_ids* is a data structure of type *ids_t*, which closely resembles
    *std::vector*. See the specific explanation below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Algorithm for MVCC ReadView visibility determination, specifically refer to
    the *changes_visible* function below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From the code, it can be seen that the visibility algorithm works efficiently
    when concurrency is low. However, as concurrency increases, the efficiency of
    using binary search to determine visibility significantly decreases, particularly
    in NUMA environments.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Solutions for Enhancing MVCC ReadView Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two fundamental approaches to improving scalability here [58]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*First, finding an algorithm that improves the complexity, so that each additional
    connection does not increase the snapshot computation costs linearly.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Second, perform less work for each connection, hopefully reducing the total
    time taken so much that even at high connection counts the total time is still
    small enough to not matter much (i.e. reduce the constant factor).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first solution, adopting a multi-version visibility algorithm based
    on Commit Sequence Numbers (CSN) offers benefits [7]: *the cost of taking snapshots
    can be reduced by converting snapshots into CSNs instead of maintaining a transaction
    ID list.* Specifically, under the Read Committed isolation level, there’s no need
    to replicate an active transaction list for each read operation, thereby improving
    scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the complexity of implementation, this book opts for the second
    solution, which directly modifies the MVCC ReadView data structure to mitigate
    MVCC ReadView scalability problems.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Improvements to the MVCC ReadView Data Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the ReadView structure, the original approach used a vector to store the
    list of active transactions. Now, it has been changed to the following data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, corresponding code modifications were made in the related interface
    functions, as changes to the data structure necessitate adjustments to the internal
    code within these functions.
  prefs: []
  type: TYPE_NORMAL
- en: This new MVCC ReadView data structure can be seen as a hybrid data structure,
    as shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a14b720b152505bef3bc6d0d0ace923.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-19\. A new hybrid data structure suitable for active transaction list
    in MVCC ReadView.
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed explanation, please refer to Chapter 4.2.8 on hybrid data
    structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, online transactions are short rather than long, and transaction
    IDs increase continuously. To leverage these characteristics, a hybrid data structure
    is used: a static array for consecutive short transaction IDs and a vector for
    long transactions. With a 2048-byte array, up to 16,384 consecutive active transaction
    IDs can be stored, each bit representing a transaction ID.'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum short transaction ID is used to differentiate between short and
    long transactions. IDs smaller than this minimum go into the long transaction
    vector, while IDs equal to or greater than it are placed in the short transaction
    array. For an ID in changes_visible, if it is below the minimum short transaction
    ID, a direct query is made to the vector, which is efficient due to the generally
    small number of long transactions. If the ID is equal to or above the minimum
    short transaction ID, a bitwise query is performed, with a time complexity of
    O(1), compared to the previous O(log n) complexity. This improvement enhances
    efficiency and reduces cache migration between NUMA nodes, as O(1) queries typically
    complete within a single CPU time slice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the previously mentioned transformation, similar modifications
    were applied to the global transaction active list. The original data structure
    used for this list is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it has been changed to the following data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the *short_rw_trx_ids_bitmap* structure, *MAX_SHORT_ACTIVE_BYTES* is set
    to 65536, theoretically accommodating up to 524,288 consecutive short transaction
    IDs. If the limit is exceeded, the oldest short transaction IDs are converted
    into long transactions and stored in *long_rw_trx_ids*. Global long and short
    transactions are distinguished by *min_short_valid_id*: IDs smaller than this
    value are treated as global long transactions, while IDs equal to or greater are
    considered global short transactions.'
  prefs: []
  type: TYPE_NORMAL
- en: During the copying process from the global active transaction list, the *short_rw_trx_ids_bitmap*
    structure, which uses only one bit per transaction ID, allows for much higher
    copying efficiency compared to the native MySQL solution. For example, with 1000
    active transactions, the native MySQL version would require copying at least 8000
    bytes, whereas the optimized solution may only need a few hundred bytes. This
    results in a significant improvement in copying efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing these modifications, performance comparison tests were conducted
    to evaluate the effectiveness of the MVCC ReadView optimization. The figure below
    shows a comparison of TPC-C throughput with varying concurrency levels, before
    and after modifying the MVCC ReadView data structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104222478](../Images/8e5e58e393f312425315ec149a3e1708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-20\. Performance comparison before and after adopting the new hybrid
    data structure in NUMA.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it is evident that this transformation primarily optimized
    scalability and improved MySQL’s peak throughput in NUMA environments. Further
    performance comparisons before and after optimization can be analyzed using tools
    like *perf*. Below is a screenshot from *perf* at 300 concurrency, prior to optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d383d113a8a0b3178e71e6eceb047d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-21\. Latch-related bottleneck observed in *perf* screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it can be seen that the first two bottlenecks were significant,
    accounting for approximately 33% of the overhead. After optimization, the *perf*
    screenshot at 300 concurrency is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b65f8f76e1a83e4082df6dcb03af4be4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-22\. Significant alleviation of latch-related bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: After optimization, as shown in the screenshot above, the proportions of the
    previous top two bottlenecks have been significantly reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Why does changing the MVCC ReadView data structure significantly enhance scalability?
    This is because accessing these structures involves acquiring a global latch.
    Optimizing the data structure accelerates access to critical resources, reducing
    concurrency conflicts and minimizing cache migration across NUMA nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The native MVCC ReadView uses a vector to store the list of active transactions.
    In high-concurrency scenarios, this list can become large, leading to a larger
    working set. In NUMA environments, both querying and replication can become slower,
    potentially causing a single CPU time slice to miss its deadline and resulting
    in significant context-switching costs. The theoretical basis for this aspect
    is as follows [21]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Context-switches that occur in the middle of a logical operation evict a possibly
    larger working set from the cache. When the suspended thread resumes execution,
    it wastes time restoring the evicted working set.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughput improvement under the ARM architecture is evaluated next. Details
    are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104512068](../Images/7d6acf6b20bf936f3a027bc456e5fae2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-23\. Throughput improvement under the ARM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is evident that there is also a significant improvement
    under the ARM architecture. Extensive test data confirms that the MVCC ReadView
    optimization yields clear benefits in NUMA environments, regardless of whether
    the architecture is ARM or x86.
  prefs: []
  type: TYPE_NORMAL
- en: How much improvement can this optimization achieve in a SMP environment?
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104533718](../Images/4bb8dae8ea938bd73dbd5cb04fc2ed9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-24\. Performance comparison before and after adopting the new hybrid
    data structure in SMP.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it can be observed that after binding to NUMA node 0, the improvement
    from the MVCC ReadView optimization is not significant. This suggests that the
    optimization primarily enhances scalability in NUMA architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In practical MySQL usage, preventing excessive user threads from entering the
    InnoDB storage engine can significantly reduce the size of the global active transaction
    list. This transaction throttling mechanism complements the MVCC ReadView optimization
    effectively, improving overall performance. Combined with double latch avoidance,
    discussed in the next section, the TPC-C test results in the following figure
    clearly demonstrate these improvements.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104554155](../Images/b76ceb837bbfe2d9e92688d834f04658.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-25\. Maximum TPC-C throughput in BenchmarkSQL with transaction throttling
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Avoiding Double Latch Problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During testing after the MVCC ReadView optimization, a noticeable decline in
    throughput was observed under extremely high concurrency conditions. The specific
    details are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104639402](../Images/2d5f5f538623a28aa1ddf03d6f16f853.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-26\. Performance degradation at concurrency Levels exceeding 500.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it can be seen that throughput significantly decreases once
    concurrency exceeds 500\. The problem was traced to frequent acquisitions of the
    *trx-sys* latch, as shown in the code snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The other code snippet is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'InnoDB introduces a global trx-sys latch during the view close process, impacting
    scalability under high concurrency. To address this, an attempt was made to remove
    the global latch. One of the modifications is shown in the code snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The other modification is shown in the code snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the MVCC ReadView optimized version, compare TPC-C throughput before
    and after the modifications. Details are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104851205](../Images/779c5517441e8d96cc3be821afab3742.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-27\. Performance improvement after eliminating the double latch bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it is evident that the modifications significantly improved
    scalability under high-concurrency conditions. To understand the reasons for this
    improvement, let’s use the *perf* tool for further investigation. Below is the
    *perf* screenshot at 2000 concurrency before the modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b695cae35d943eedce4428ea53ddbc14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-28\. Latch-related bottleneck observed in *perf* screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it is evident that the latch-related bottlenecks are quite
    pronounced. After the code modifications, here is the *perf* screenshot at 3000
    concurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61b5194615f9be3584a0c4db467ccbaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-29\. Significant alleviation of latch-related bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Even with higher concurrency, such as 3000, the bottlenecks are not pronounced.
    This suggests that the optimizations have effectively alleviated the latch-related
    performance problems, improving scalability under extreme conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Excluding the global latch before and after the *view_close* function call improves
    scalability, while including it severely degrades scalability under high concurrency.
    Although the *view_close* function operates efficiently within its critical section,
    frequent acquisition of the globally used *trx-sys* latch—used throughout the
    *trx-sys* subsystem—causes significant contention and head-of-line blocking. This
    issue, known as the ‘double latch’ problem, arises from both *view_open* and *view_close*
    requiring the global *trx-sys* latch. Notably, removing the latch from the final
    stage or using a new latch can significantly mitigate this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.5 Explaining the Super-Linear Performance Phenomenon
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Section 2.1 describes super-linear scaling in throughput observed during SysBench
    read-write tests in an x86 NUMA environment. Following improvements to the InnoDB
    storage engine, the current investigation examines whether this super-linear scaling
    effect remains. Tests were conducted in the same environment using the improved
    version of MySQL. The results of individual SysBench tests for MySQL instance
    1 and MySQL instance 2 are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104924497](../Images/7323d9486156900a9a7b099201997433.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-30\. Throughput of MySQL running separately after MVCC optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The throughput for each instance has significantly improved, with MySQL instance
    1 achieving 524,381 QPS and MySQL instance 2 reaching 553,008 QPS. Combined, they
    deliver 1,077,389 QPS, substantially surpassing the previous 328,168 QPS.
  prefs: []
  type: TYPE_NORMAL
- en: SysBench is used to simultaneously evaluate the read-write performance of these
    two instances, as illustrated in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829104945257](../Images/a809a96686d28e7c97a28b3b4ecc16fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-31\. Throughput of MySQL running together after MVCC optimization.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, one instance achieves a throughput of 289,702 QPS while the
    other reaches 285,026 QPS. The combined throughput of both MySQL instances totals
    574,728 QPS, closely aligning with the 546,429 QPS observed with the MySQL release
    version. This illustrates the Linux operating system’s role in effectively scheduling
    multiple processes in NUMA environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the data, it’s evident that the total throughput of two MySQL instances
    running simultaneously is significantly lower than the throughput of each instance
    running individually. For detailed statistical comparisons, refer to the figure
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829105004347](../Images/1b95d5e8c2456dd67560f1341385874c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-32\. Total throughput of running separately vs. running together after
    MVCC optimization.
  prefs: []
  type: TYPE_NORMAL
- en: After enhancements to the InnoDB storage engine, the super-linear scaling problem
    has been solved, revealing that the underlying cause in the MySQL release version
    was a poorly designed MVCC mechanism within InnoDB. This design flaw amplified
    problems in NUMA environments, resulting in the observed super-linear scaling
    effect.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Transaction Throttling Mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to the paper “Staring into the Abyss: An Evaluation of Concurrency
    Control with One Thousand Cores” [22], centralized databases, including MySQL,
    struggle to fully utilize hundreds of CPU cores.'
  prefs: []
  type: TYPE_NORMAL
- en: To address scalability problems, traditional approaches use thread pools to
    restrict the number of CPU cores a database utilizes. This book introduces a transaction
    throttling mechanism that limits the number of threads accessing the transaction
    system, offering an alternative method to mitigate scalability challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Percona Thread Pool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In general, a thread pool in traditional MySQL serves two main purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mitigating Short Connection Storms**: By managing and reusing threads, the
    thread pool helps prevent system overload during sudden spikes in short-lived
    connections.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Enhancing Scalability**: Thread pools improve scalability, particularly in
    high-contention scenarios, by enabling MySQL to more effectively utilize available
    CPU cores.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Percona’s thread pool as a case study, let’s examine the cost-effectiveness
    of thread pools in improving MySQL scalability. The following figure compares
    throughput and concurrency before and after implementing a thread pool with the
    improved version of MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829105130499](../Images/4170aa73f79f39dd66136a6331d2010b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-33\. Enabling the Percona thread pool led to a noticeable decrease
    in throughput.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it is evident that throughput decreases after adopting the
    thread pool. This decline is attributed to the high inherent cost of the Percona
    thread pool. Moreover, with the improved version of MySQL already achieving significant
    scalability improvements, the additional benefits of the Percona thread pool in
    improving scalability for TPC-C applications are limited.
  prefs: []
  type: TYPE_NORMAL
- en: For MySQL versions with poor scalability, the thread pool remains valuable in
    addressing scalability problems. As demonstrated in Chapter 1, the use of the
    thread pool significantly alleviated scalability problems in MySQL 5.7.
  prefs: []
  type: TYPE_NORMAL
- en: Based on extensive testing, after solving most of MySQL’s scalability problems,
    it was observed that while the Percona thread pool can still be effective in managing
    short-lived connections and high contention scenarios, its overall effectiveness
    diminishes and it may even hinder performance in other contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Transaction Throttling Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Centralized databases struggle to fully utilize hundreds of CPU cores due to
    limitations in their transaction systems. To address this, transaction throttling
    mechanisms are becoming increasingly important.
  prefs: []
  type: TYPE_NORMAL
- en: MySQL has introduced a “Max Transaction Limit” feature in its thread pool to
    mitigate performance degradation [31]. This feature limits the number of concurrent
    transactions, improving throughput by reducing data locks and deadlocks on heavily
    loaded systems [64]. This approach can inspire similar mechanisms that increase
    throughput in high-concurrency scenarios without relying solely on traditional
    thread pools.
  prefs: []
  type: TYPE_NORMAL
- en: 'For MySQL, the specific process figure for transaction throttling is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63a4b5f8a5215a183c98014d20f80cab.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-34\. New transaction throttling mechanism In MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting a new transaction, check if the number of concurrent transactions
    exceeds the limit. If it does, the transaction enters a waiting state. Otherwise,
    it proceeds into the transaction system. After a transaction completes execution,
    a user thread from the waiting list is activated to continue executing another
    transaction.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing this transaction throttling mechanism, MySQL’s scalability
    is validated. The following figure illustrates the relationship between TPC-C
    throughput and concurrency when using the throttling mechanism compared to using
    the Percona thread pool.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829105242300](../Images/1c0e147423c310c208469e6c855764d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-35\. Impact of the transaction throttling mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it can be seen that the throttling approach is superior to
    the Percona thread pool approach, and this superiority is comprehensive.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure depicts the TPC-C scalability stress test conducted after
    implementing transaction throttling. The test was performed in a scenario with
    NUMA BIOS disabled, limiting entry of up to 512 user threads into the transaction
    system.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-20240829105258689](../Images/2f74bf5b55d5674c4eed2cc60720f9a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-36\. Maximum TPC-C throughput in BenchmarkSQL with transaction throttling
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it can be seen that throughput is more stable. This stability
    is primarily due to disabling NUMA in the BIOS, which improves memory access efficiency
    and enhances overall system stability.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, transaction throttling is not a panacea and has its limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: When the maximum number of transactions are executing concurrently, new transactions
    must wait until existing transactions are completed. If all concurrent transactions
    consist of long-running queries, it may appear as if the MySQL system is stalled
    [31].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s worth noting that the specifics of how transaction throttling is implemented,
    and its flexibility, are areas where AI can demonstrate its usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Mitigating the Increasing Performance Decline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Users tend to notice a decline in low-concurrency performance more easily, while
    improvements in high-concurrency performance are often harder to perceive. Therefore,
    maintaining low-concurrency performance is crucial, as it directly affects user
    experience and the willingness to upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: According to extensive user feedback, after upgrading to MySQL 8.0, users have
    generally perceived a decline in performance, particularly in batch insert and
    join operations. This downward trend has become more evident in higher versions
    of MySQL. Additionally, some MySQL enthusiasts and testers have reported performance
    degradation in multiple sysbench tests after upgrading.
  prefs: []
  type: TYPE_NORMAL
- en: Can these performance issues be avoided? Or, more specifically, how should we
    scientifically assess the ongoing trend of performance decline? These are important
    questions to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Although the official team continues to optimize, the gradual deterioration
    of performance cannot be overlooked. In certain scenarios, there may appear to
    be improvements, but this does not mean that performance in all scenarios is equally
    optimized. Moreover, it’s also easy to optimize performance for specific scenarios
    at the cost of degrading performance in other areas.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 The Root Causes of MySQL Performance Decline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, as more features are added, the codebase grows, and with the continuous
    expansion of functionality, performance becomes increasingly difficult to control.
  prefs: []
  type: TYPE_NORMAL
- en: MySQL developers often fail to notice the decline in performance, as each addition
    to the codebase results in only a very small decrease in performance. However,
    over time, these small declines accumulate, leading to a significant cumulative
    effect, which causes users to perceive a noticeable performance degradation in
    newer versions of MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following figure shows the performance of a simple single
    join operation, with MySQL 8.0.40 showing a performance decline compared to MySQL
    8.0.27:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-join-degrade](../Images/33ce04c643f3855aea7871e389914908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-37\. Significant decline in join performance in MySQL 8.0.40.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the batch insert performance test under single concurrency,
    with the performance decline of MySQL 8.0.40 compared to version 5.7.44:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image-bulk-insert-degrade](../Images/56ffd9da7183234125d6a5ab8e0ee6f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-38\. Significant decline in bulk insert performance in MySQL 8.0.40.
  prefs: []
  type: TYPE_NORMAL
- en: From the two graphs above, it can be seen that the performance of version 8.0.40
    is not good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s analyze the root cause of the performance degradation in MySQL
    from the code level. Below is the ***PT_insert_values_list::contextualize*** function
    in MySQL 8.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding ***PT_insert_values_list::contextualize*** function in MySQL
    5.7 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: From the code comparison, MySQL 8.0 appears to have more elegant code, seemingly
    making progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, many times it is precisely the motivations behind these code
    improvements that lead to performance degradation. The MySQL official team replaced
    the previous ***List*** data structure with a ***deque***, which has become one
    of the root causes of the gradual performance degradation. Let’s take a look at
    the ***deque*** documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the above description, in extreme cases, retaining a single element
    requires allocating the entire array, resulting in very low memory efficiency.
    For example, in bulk inserts, where a large number of records need to be inserted,
    the official implementation stores each record in a separate deque. Even if the
    record content is minimal, a deque must still be allocated. The MySQL deque implementation
    allocates 1KB of memory for each deque to support fast lookups.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The official implementation uses 1KB of memory to store index information, and
    even if the record length is not large but there are many records, the memory
    access addresses may become non-contiguous, leading to poor cache friendliness.
    This design was intended to improve cache friendliness, but it has not been fully
    effective.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the original implementation used a List data structure,
    where memory was allocated through a memory pool, providing a certain level of
    cache friendliness. Although random access is less efficient, optimizing for sequential
    access to List elements significantly improves performance.
  prefs: []
  type: TYPE_NORMAL
- en: During the upgrade to MySQL 8.0, users observed a significant decline in batch
    insert performance, and one of the main causes was the substantial change in underlying
    data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, while the official team improved the redo log mechanism, this
    also led to a decrease in MTR commit operation efficiency. Compared to MySQL 5.7,
    the added code significantly reduces the performance of individual commits, even
    though overall write throughput has been greatly improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the core ***execute*** operation of MTR commit in MySQL 5.7.44:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine the core ***execute*** operation of MTR commit in MySQL 8.0.40:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: By comparison, it is clear that in MySQL 8.0.40, the execute operation in MTR
    commit has become much more complex, with more steps involved. This complexity
    is one of the main causes of the decline in low-concurrency write performance.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the operations ***m_impl->m_log.for_each_block(write_log)***
    and **log_wait_for_space_in_log_recent_closed(*log_sys, handle.start_lsn)** have
    significant overhead. These changes were made to enhance high-concurrency performance,
    but they came at the cost of low-concurrency performance.
  prefs: []
  type: TYPE_NORMAL
- en: The redo log’s prioritization of high-concurrency mode results in poor performance
    for low-concurrency workloads. Although the introduction of ***innodb_log_writer_threads***
    was intended to mitigate low-concurrency performance issues, it does not affect
    the execution of the above functions. Since these operations have become more
    complex and require frequent MTR commits, performance has still dropped significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the impact of the instant add/drop feature on performance.
    Below is the ***rec_init_offsets_comp_ordinary*** function in MySQL 5.7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The ***rec_init_offsets_comp_ordinary*** function in MySQL 8.0.40 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: From the above code, it is clear that with the introduction of the instant add/drop
    column feature, the ***rec_init_offsets_comp_ordinary*** function has become noticeably
    more complex, introducing more function calls and adding a switch statement that
    severely impacts cache optimization. Since this function is called frequently,
    it directly impacts the performance of update index, batch inserts, and joins,
    resulting in a major performance hit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the performance decline in MySQL 8.0 is not limited to the above;
    there are many other areas that contribute to the overall performance degradation,
    especially the impact on the expansion of inline functions. For example, the following
    code affects the expansion of inline functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: According to our tests, the ***ib::fatal*** statement severely interferes with
    inline optimization. For frequently accessed functions, it is advisable to avoid
    statements that interfere with inline optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s look at a similar issue. The ***row_sel_store_mysql_field function***
    is called frequently, with ***row_sel_field_store_in_mysql_format*** being a hotspot
    function within it. The specific code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The ***row_sel_field_store_in_mysql_format*** function ultimately calls ***row_sel_field_store_in_mysql_format_func***.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The ***row_sel_field_store_in_mysql_format_func*** function cannot be inlined
    due to the presence of the ***ib::fatal*** code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Frequently called inefficient functions, executing tens of millions of times
    per second, can severely impact join performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue to explore the reasons for performance decline. The following
    official performance optimization is, in fact, one of the root causes of the decline
    in join performance. Although certain queries may be improved, it is still one
    of the reasons for the performance degradation of ordinary join operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: MySQL’s issues go beyond these. As shown in the analyses above, the performance
    decline in MySQL is not without cause. A series of small problems, when accumulated,
    can lead to noticeable performance degradation that users experience. However,
    these issues are often difficult to identify, making them even harder to resolve.
  prefs: []
  type: TYPE_NORMAL
- en: The so-called ‘premature optimization’ is the root of all evil, and it does
    not apply in MySQL development. Database development is a complex process, and
    neglecting performance over time makes subsequent performance improvements significantly
    more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Solutions to Mitigate MySQL Performance Decline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main reasons for the decline in write performance are related to MTR commit
    issues, instant add/drop column, and several other factors. These are difficult
    to optimize in traditional ways. However, users can compensate for the performance
    drop through PGO optimization. With a proper strategy, the performance can generally
    be kept stable.
  prefs: []
  type: TYPE_NORMAL
- en: For batch insert performance degradation, our open-source version [64] replaces
    the official deque with an improved list implementation. This primarily addresses
    memory efficiency issues and can partially alleviate performance decline. By combining
    PGO optimization with our open-source version, batch insert performance can approach
    that of MySQL 5.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-bulk-insert-optimize.png](../Images/1293309958c11193c6ec32397c4297e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-39\. Optimized MySQL 8.0.40 with PGO performs roughly on par with version
    5.7.
  prefs: []
  type: TYPE_NORMAL
- en: Users can also leverage multiple threads for concurrent batch processing, fully
    utilizing the improved concurrency of the redo log, which can significantly boost
    batch insert performance.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding update index issues, due to the inevitable addition of new code, PGO
    optimization can help mitigate this problem. Our PGO version [64] can significantly
    alleviate this issue.
  prefs: []
  type: TYPE_NORMAL
- en: For read performance, particularly join performance, we have made substantial
    improvements, including fixing inline issues and making other optimizations. With
    the addition of PGO, join performance can be increased by over 30% compared to
    the official version.
  prefs: []
  type: TYPE_NORMAL
- en: '![image-join-improve.png](../Images/b385c9506ba11f9329562a3f07aa25f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-40\. Join performance optimization with PGO leads to significant improvements.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to invest time in optimizing low-concurrency performance. This
    process is long but involves numerous areas that need improvement.
  prefs: []
  type: TYPE_NORMAL
- en: The open-source version is available for testing, and efforts will persist to
    improve MySQL performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[Next](/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/Chapter9.html)'
  prefs: []
  type: TYPE_NORMAL
